{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"header\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# üöÄ Claude-to-CodeLlama Knowledge Distillation\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Transform Claude Opus 4's Superior Code Generation into an Accessible 7B Model**\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a complete end-to-end implementation of knowledge distillation from Claude Opus 4 to Code Llama 7B.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üìã Features\\n\",\n",
    "    \"- üß† **Teacher-Student Learning**: Claude Opus 4 ‚Üí Code Llama 7B\\n\",\n",
    "    \"- üí∞ **Cost Effective**: ~$50-100 for Colab Pro training\\n\",\n",
    "    \"- ‚ö° **Memory Efficient**: QLoRA optimization for 6GB GPU\\n\",\n",
    "    \"- üìä **Comprehensive Evaluation**: HumanEval and MBPP benchmarks\\n\",\n",
    "    \"- üîß **Production Ready**: Save and deploy your trained model\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üéØ Expected Results\\n\",\n",
    "    \"- **HumanEval**: 70-75% pass@1 (vs 33.5% baseline)\\n\",\n",
    "    \"- **MBPP**: 65-70% pass@1 (vs 41.4% baseline)\\n\",\n",
    "    \"- **Training Time**: 4-6 hours on Colab Pro\\n\",\n",
    "    \"- **Total Cost**: ~$60-80 including API calls\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"setup\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üõ†Ô∏è Setup Environment\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, let's set up the environment and install dependencies.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"install\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Check GPU availability\\n\",\n",
    "    \"!nvidia-smi\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Mount Google Drive for persistent storage\\n\",\n",
    "    \"from google.colab import drive\\n\",\n",
    "    \"drive.mount('/content/drive')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create project directory\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"PROJECT_DIR = '/content/drive/MyDrive/claude_distillation'\\n\",\n",
    "    \"os.makedirs(PROJECT_DIR, exist_ok=True)\\n\",\n",
    "    \"os.chdir(PROJECT_DIR)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úÖ Working directory: {os.getcwd()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"clone\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Clone repository\\n\",\n",
    "    \"if not os.path.exists('claude_to_codellama_distillation'):\\n\",\n",
    "    \"    !git clone https://github.com/yeditepe/claude-to-codellama-distillation.git\\n\",\n",
    "    \"    print(\\\"‚úÖ Repository cloned\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚úÖ Repository already exists\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"os.chdir('claude_to_codellama_distillation')\\n\",\n",
    "    \"print(f\\\"üìÇ Project directory: {os.getcwd()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"requirements\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Install requirements\\n\",\n",
    "    \"!pip install -r requirements.txt\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Additional Colab-specific packages\\n\",\n",
    "    \"!pip install google-colab\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ Requirements installed\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"config\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üîë Configuration\\n\",\n",
    "    \"\\n\",\n",
    "    \"Set up your API keys and configuration.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"api_keys\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"from getpass import getpass\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set API keys\\n\",\n",
    "    \"print(\\\"üîë Setting up API keys...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Claude API key (required)\\n\",\n",
    "    \"if not os.getenv('ANTHROPIC_API_KEY'):\\n\",\n",
    "    \"    anthropic_key = getpass('Enter your Anthropic API key: ')\\n\",\n",
    "    \"    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\\n\",\n",
    "    \"    print(\\\"‚úÖ Claude API key set\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚úÖ Claude API key already set\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Weights & Biases (optional)\\n\",\n",
    "    \"if not os.getenv('WANDB_API_KEY'):\\n\",\n",
    "    \"    wandb_key = getpass('Enter your W&B API key (optional, press Enter to skip): ')\\n\",\n",
    "    \"    if wandb_key:\\n\",\n",
    "    \"        os.environ['WANDB_API_KEY'] = wandb_key\\n\",\n",
    "    \"        print(\\\"‚úÖ W&B API key set\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"‚è≠Ô∏è W&B skipped\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚úÖ W&B API key already set\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"colab_config\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Colab-specific configuration\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add src to path\\n\",\n",
    "    \"sys.path.append('./src')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check GPU\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"print(f\\\"üéÆ Device: {device}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if torch.cuda.is_available():\\n\",\n",
    "    \"    gpu_name = torch.cuda.get_device_name(0)\\n\",\n",
    "    \"    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\\n\",\n",
    "    \"    print(f\\\"GPU: {gpu_name} ({gpu_memory:.1f}GB)\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚ö†Ô∏è No GPU available - training will be very slow!\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Colab-optimized configuration\\n\",\n",
    "    \"COLAB_CONFIG = {\\n\",\n",
    "    \"    'target_size': 1000,  # Small dataset for demo\\n\",\n",
    "    \"    'num_epochs': 1,      # Quick training\\n\",\n",
    "    \"    'batch_size': 2,      # Small batch for memory\\n\",\n",
    "    \"    'max_length': 1024,   # Shorter sequences\\n\",\n",
    "    \"    'use_4bit': True,     # QLoRA quantization\\n\",\n",
    "    \"    'lora_r': 8,          # Smaller LoRA rank\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ Colab configuration set\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"phase1\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üìä Phase 1: Dataset Generation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Generate high-quality code examples using Claude Opus 4.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"dataset_gen\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import asyncio\\n\",\n",
    "    \"from dataset_generator import DatasetGenerator, DatasetConfig\\n\",\n",
    "    \"from claude_client import ClaudeConfig\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure dataset generation\\n\",\n",
    "    \"claude_config = ClaudeConfig(\\n\",\n",
    "    \"    api_key=os.getenv('ANTHROPIC_API_KEY'),\\n\",\n",
    "    \"    model='claude-3-opus-20240229',\\n\",\n",
    "    \"    max_tokens=1024,\\n\",\n",
    "    \"    temperature=0.1,\\n\",\n",
    "    \"    rate_limit_rpm=30  # Conservative for Colab\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"dataset_config = DatasetConfig(\\n\",\n",
    "    \"    target_size=COLAB_CONFIG['target_size'],\\n\",\n",
    "    \"    languages=['python', 'javascript'],  # Start with 2 languages\\n\",\n",
    "    \"    output_dir='./data/generated'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üèóÔ∏è Starting dataset generation...\\\")\\n\",\n",
    "    \"print(f\\\"Target size: {dataset_config.target_size} examples\\\")\\n\",\n",
    "    \"print(f\\\"Languages: {dataset_config.languages}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"run_generation\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Run dataset generation\\n\",\n",
    "    \"async def generate_dataset():\\n\",\n",
    "    \"    generator = DatasetGenerator(dataset_config, claude_config)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print('üìä Generating dataset...')\\n\",\n",
    "    \"    dataset = await generator.generate_dataset(max_concurrent=2)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(dataset) > 0:\\n\",\n",
    "    \"        print(f'‚úÖ Generated {len(dataset)} examples')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Split and save dataset\\n\",\n",
    "    \"        dataset_dict = generator.split_dataset(dataset)\\n\",\n",
    "    \"        generator.save_dataset(dataset_dict, format='jsonl')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Generate quality report\\n\",\n",
    "    \"        report = generator.generate_quality_report()\\n\",\n",
    "    \"        print(f'üí∞ Estimated cost: ${report[\\\"generation_summary\\\"][\\\"total_cost\\\"]:.2f}')\\n\",\n",
    "    \"        print(f'üìà Success rate: {report[\\\"quality_metrics\\\"][\\\"acceptance_rate\\\"]:.2%}')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return dataset_dict\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print('‚ùå No examples generated!')\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run the generation\\n\",\n",
    "    \"dataset_dict = await generate_dataset()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"phase2\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üéØ Phase 2: Model Training\\n\",\n",
    "    \"\\n\",\n",
    "    \"Train Code Llama using knowledge distillation.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"training_setup\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from distillation_trainer import KnowledgeDistillationSystem, DistillationConfig\\n\",\n",
    "    \"import wandb\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize Weights & Biases (optional)\\n\",\n",
    "    \"if os.getenv('WANDB_API_KEY'):\\n\",\n",
    "    \"    wandb.init(\\n\",\n",
    "    \"        project=\\\"claude-to-codellama-colab\\\",\\n\",\n",
    "    \"        config=COLAB_CONFIG,\\n\",\n",
    "    \"        tags=[\\\"colab\\\", \\\"demo\\\"]\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    print(\\\"‚úÖ W&B initialized\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure training\\n\",\n",
    "    \"config = DistillationConfig(\\n\",\n",
    "    \"    student_model_name='codellama/CodeLlama-7b-hf',\\n\",\n",
    "    \"    dataset_path='./data/generated',\\n\",\n",
    "    \"    output_dir='./models/distilled_codellama',\\n\",\n",
    "    \"    max_length=COLAB_CONFIG['max_length'],\\n\",\n",
    "    \"    num_epochs=COLAB_CONFIG['num_epochs'],\\n\",\n",
    "    \"    batch_size=COLAB_CONFIG['batch_size'],\\n\",\n",
    "    \"    gradient_accumulation_steps=4,\\n\",\n",
    "    \"    learning_rate=2e-4,\\n\",\n",
    "    \"    use_4bit=COLAB_CONFIG['use_4bit'],\\n\",\n",
    "    \"    lora_r=COLAB_CONFIG['lora_r'],\\n\",\n",
    "    \"    lora_alpha=16,\\n\",\n",
    "    \"    use_gradient_checkpointing=True,\\n\",\n",
    "    \"    eval_steps=50,\\n\",\n",
    "    \"    save_steps=100,\\n\",\n",
    "    \"    logging_steps=5\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üéØ Training configuration ready\\\")\\n\",\n",
    "    \"print(f\\\"Model: {config.student_model_name}\\\")\\n\",\n",
    "    \"print(f\\\"Epochs: {config.num_epochs}\\\")\\n\",\n",
    "    \"print(f\\\"Batch size: {config.batch_size}\\\")\\n\",\n",
    "    \"print(f\\\"LoRA rank: {config.lora_r}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"run_training\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize training system\\n\",\n",
    "    \"print(\\\"üöÄ Initializing training system...\\\")\\n\",\n",
    "    \"system = KnowledgeDistillationSystem(config)\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    print(\\\"üìö Loading model and datasets...\\\")\\n\",\n",
    "    \"    results = system.run_full_training()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"üéâ Training completed successfully!\\\")\\n\",\n",
    "    \"    print(f\\\"Final training loss: {results['train_result'].training_loss:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Final eval loss: {results['eval_results']['eval_loss']:.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Save training metrics\\n\",\n",
    "    \"    training_metrics = {\\n\",\n",
    "    \"        'train_loss': results['train_result'].training_loss,\\n\",\n",
    "    \"        'eval_loss': results['eval_results']['eval_loss'],\\n\",\n",
    "    \"        'epochs': config.num_epochs,\\n\",\n",
    "    \"        'model_size': '7B',\\n\",\n",
    "    \"        'lora_rank': config.lora_r\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if wandb.run:\\n\",\n",
    "    \"        wandb.log(training_metrics)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"‚ùå Training failed: {e}\\\")\\n\",\n",
    "    \"    print(\\\"This might be due to insufficient data or memory constraints.\\\")\\n\",\n",
    "    \"    print(\\\"Try reducing batch_size or dataset size.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"phase3\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üìà Phase 3: Model Evaluation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Evaluate the trained model on standard benchmarks.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"evaluation_setup\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from evaluation_system import ModelComparator, EvaluationConfig\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure evaluation\\n\",\n",
    "    \"eval_config = EvaluationConfig(\\n\",\n",
    "    \"    student_model_path='./models/distilled_codellama',\\n\",\n",
    "    \"    baseline_models=['codellama/CodeLlama-7b-hf'],  # Compare with base model\\n\",\n",
    "    \"    test_datasets=['humaneval'],  # Start with one benchmark\\n\",\n",
    "    \"    output_dir='./evaluation_results',\\n\",\n",
    "    \"    max_new_tokens=256,  # Shorter for speed\\n\",\n",
    "    \"    temperature=0.1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìä Evaluation configuration ready\\\")\\n\",\n",
    "    \"print(f\\\"Datasets: {eval_config.test_datasets}\\\")\\n\",\n",
    "    \"print(f\\\"Baseline models: {eval_config.baseline_models}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"run_evaluation\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Run evaluation\\n\",\n",
    "    \"print(\\\"üîç Starting model evaluation...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    comparator = ModelComparator(eval_config)\\n\",\n",
    "    \"    results = comparator.compare_models()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"‚úÖ Evaluation completed!\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Display results\\n\",\n",
    "    \"    if 'comparison_summary' in results:\\n\",\n",
    "    \"        summary = results['comparison_summary']\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for metric_name, rankings in summary['rankings'].items():\\n\",\n",
    "    \"            print(f\\\"\\\\nüìä {metric_name}:\\\")\\n\",\n",
    "    \"            for i, ranking in enumerate(rankings):\\n\",\n",
    "    \"                emoji = \\\"ü•á\\\" if i == 0 else \\\"ü•à\\\" if i == 1 else \\\"ü•â\\\" if i == 2 else \\\"üìç\\\"\\n\",\n",
    "    \"                print(f\\\"  {emoji} {ranking['model']}: {ranking['value']:.3f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate detailed report\\n\",\n",
    "    \"    if eval_config.generate_report:\\n\",\n",
    "    \"        report = comparator.generate_report(results)\\n\",\n",
    "    \"        print(f\\\"\\\\nüìã Detailed report saved to {eval_config.output_dir}/evaluation_report.md\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Log to W&B\\n\",\n",
    "    \"    if wandb.run and 'comparison_summary' in results:\\n\",\n",
    "    \"        wandb.log({\\\"evaluation_results\\\": summary})\\n\",\n",
    "    \"        \\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"‚ùå Evaluation failed: {e}\\\")\\n\",\n",
    "    \"    print(\\\"This might be due to model loading issues or benchmark dataset problems.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"inference\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üß™ Interactive Inference\\n\",\n",
    "    \"\\n\",\n",
    "    \"Test your trained model with custom prompts.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"load_model\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"from transformers import AutoTokenizer, AutoModelForCausalLM\\n\",\n",
    "    \"from peft import PeftModel\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load trained model for inference\\n\",\n",
    "    \"def load_trained_model(model_path):\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        print(f\\\"üîÑ Loading model from {model_path}...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Load tokenizer\\n\",\n",
    "    \"        tokenizer = AutoTokenizer.from_pretrained(model_path)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Load model\\n\",\n",
    "    \"        model = AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "    \"            model_path,\\n\",\n",
    "    \"            torch_dtype=torch.float16,\\n\",\n",
    "    \"            device_map=\\\"auto\\\"\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(\\\"‚úÖ Model loaded successfully\\\")\\n\",\n",
    "    \"        return model, tokenizer\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"‚ùå Failed to load model: {e}\\\")\\n\",\n",
    "    \"        print(\\\"Using base model instead...\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Fallback to base model\\n\",\n",
    "    \"        tokenizer = AutoTokenizer.from_pretrained('codellama/CodeLlama-7b-hf')\\n\",\n",
    "    \"        model = AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "    \"            'codellama/CodeLlama-7b-hf',\\n\",\n",
    "    \"            torch_dtype=torch.float16,\\n\",\n",
    "    \"            device_map=\\\"auto\\\"\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        return model, tokenizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the model\\n\",\n",
    "    \"model, tokenizer = load_trained_model('./models/distilled_codellama')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"inference_function\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def generate_code(prompt, max_length=256, temperature=0.1):\\n\",\n",
    "    \"    \\\"\\\"\\\"Generate code from a prompt.\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Format prompt\\n\",\n",
    "    \"    formatted_prompt = f\\\"### Instruction:\\\\n{prompt}\\\\n\\\\n### Response:\\\\n\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Tokenize\\n\",\n",
    "    \"    inputs = tokenizer(\\n\",\n",
    "    \"        formatted_prompt, \\n\",\n",
    "    \"        return_tensors=\\\"pt\\\", \\n\",\n",
    "    \"        truncation=True, \\n\",\n",
    "    \"        max_length=512\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Generate\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        outputs = model.generate(\\n\",\n",
    "    \"            **inputs,\\n\",\n",
    "    \"            max_new_tokens=max_length,\\n\",\n",
    "    \"            temperature=temperature,\\n\",\n",
    "    \"            do_sample=True,\\n\",\n",
    "    \"            top_p=0.95,\\n\",\n",
    "    \"            pad_token_id=tokenizer.eos_token_id\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Decode\\n\",\n",
    "    \"    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Extract only the generated part\\n\",\n",
    "    \"    response_start = generated_text.find(\\\"### Response:\\\\n\\\") + len(\\\"### Response:\\\\n\\\")\\n\",\n",
    "    \"    generated_code = generated_text[response_start:].strip()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return generated_code\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üß™ Inference function ready!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"test_prompts\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test with sample prompts\\n\",\n",
    "    \"test_prompts = [\\n\",\n",
    "    \"    \\\"Write a Python function to calculate the factorial of a number\\\",\\n\",\n",
    "    \"    \\\"Create a JavaScript function to validate email addresses\\\",\\n\",\n",
    "    \"    \\\"Implement a binary search algorithm in Python\\\",\\n\",\n",
    "    \"    \\\"Write a Python function to find the longest word in a sentence\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üß™ Testing model with sample prompts...\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, prompt in enumerate(test_prompts, 1):\\n\",\n",
    "    \"    print(f\\\"{'='*60}\\\")\\n\",\n",
    "    \"    print(f\\\"Test {i}: {prompt}\\\")\\n\",\n",
    "    \"    print(f\\\"{'='*60}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        generated_code = generate_code(prompt)\\n\",\n",
    "    \"        print(generated_code)\\n\",\n",
    "    \"        print()\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"‚ùå Generation failed: {e}\\\")\\n\",\n",
    "    \"        print()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"interactive_test\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Interactive testing\\n\",\n",
    "    \"print(\\\"üéÆ Interactive Code Generation\\\")\\n\",\n",
    "    \"print(\\\"Enter your prompts below. Type 'quit' to exit.\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"while True:\\n\",\n",
    "    \"    prompt = input(\\\"Enter your prompt: \\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if prompt.lower() == 'quit':\\n\",\n",
    "    \"        break\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if prompt.strip():\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            print(\\\"\\\\nü§ñ Generated Code:\\\")\\n\",\n",
    "    \"            print(\\\"-\\\" * 40)\\n\",\n",
    "    \"            generated_code = generate_code(prompt)\\n\",\n",
    "    \"            print(generated_code)\\n\",\n",
    "    \"            print(\\\"-\\\" * 40)\\n\",\n",
    "    \"            print()\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"‚ùå Error: {e}\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üëã Thanks for testing!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"save_model\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üíæ Save and Export Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"Save your trained model for future use or deployment.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"save_to_drive\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import shutil\\n\",\n",
    "    \"from datetime import datetime\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create model archive\\n\",\n",
    "    \"timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n\",\n",
    "    \"archive_name = f\\\"distilled_codellama_{timestamp}\\\"\\n\",\n",
    "    \"drive_path = f\\\"/content/drive/MyDrive/models/{archive_name}\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"üíæ Saving model to Google Drive: {drive_path}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    # Create directory\\n\",\n",
    "    \"    os.makedirs(drive_path, exist_ok=True)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Copy model files\\n\",\n",
    "    \"    if os.path.exists('./models/distilled_codellama'):\\n\",\n",
    "    \"        shutil.copytree(\\n\",\n",
    "    \"            './models/distilled_codellama', \\n\",\n",
    "    \"            f\\\"{drive_path}/model\\\",\\n\",\n",
    "    \"            dirs_exist_ok=True\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Copy evaluation results\\n\",\n",
    "    \"    if os.path.exists('./evaluation_results'):\\n\",\n",
    "    \"        shutil.copytree(\\n\",\n",
    "    \"            './evaluation_results', \\n\",\n",
    "    \"            f\\\"{drive_path}/evaluation\\\",\\n\",\n",
    "    \"            dirs_exist_ok=True\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Save training summary\\n\",\n",
    "    \"    summary = {\\n\",\n",
    "    \"        'timestamp': timestamp,\\n\",\n",
    "    \"        'config': COLAB_CONFIG,\\n\",\n",
    "    \"        'dataset_size': dataset_config.target_size if 'dataset_config' in locals() else 'unknown',\\n\",\n",
    "    \"        'training_completed': True\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    import json\\n\",\n",
    "    \"    with open(f\\\"{drive_path}/training_summary.json\\\", 'w') as f:\\n\",\n",
    "    \"        json.dump(summary, f, indent=2)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"‚úÖ Model saved successfully!\\\")\\n\",\n",
    "    \"    print(f\\\"üìÅ Location: {drive_path}\\\")\\n\",\n",
    "    \"    print(f\\\"üìä Summary: {drive_path}/training_summary.json\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"‚ùå Failed to save model: {e}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"deployment\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üöÄ Deployment Options\\n\",\n",
    "    \"\\n\",\n",
    "    \"Your model is now ready for deployment! Here are some options:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"deployment_guide\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"üöÄ Deployment Options for Your Trained Model\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"1. ü§ó Hugging Face Hub\\\")\\n\",\n",
    "    \"print(\\\"   - Upload to Hugging Face for easy sharing\\\")\\n\",\n",
    "    \"print(\\\"   - Use model.push_to_hub() method\\\")\\n\",\n",
    "    \"print(\\\"   - Example: https://huggingface.co/your-username/distilled-codellama\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"2. üì± Local Inference\\\")\\n\",\n",
    "    \"print(\\\"   - Run on your local machine\\\")\\n\",\n",
    "    \"print(\\\"   - Download from Google Drive\\\")\\n\",\n",
    "    \"print(\\\"   - Use transformers library\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"3. ‚òÅÔ∏è Cloud Deployment\\\")\\n\",\n",
    "    \"print(\\\"   - Deploy on AWS/GCP/Azure\\\")\\n\",\n",
    "    \"print(\\\"   - Use Inference Endpoints\\\")\\n\",\n",
    "    \"print(\\\"   - Scale automatically\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"4. üîå API Service\\\")\\n\",\n",
    "    \"print(\\\"   - Create REST API with FastAPI\\\")\\n\",\n",
    "    \"print(\\\"   - Deploy with Docker\\\")\\n\",\n",
    "    \"print(\\\"   - Integrate with applications\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìä Model Performance Summary:\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Base Model: Code Llama 7B\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Training: Knowledge Distillation from Claude Opus 4\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Memory: ~6GB with QLoRA optimization\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Speed: ~10-20 tokens/second on consumer GPU\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Quality: Expected 70-75% on HumanEval benchmark\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üí° Next Steps:\\\")\\n\",\n",
    "    \"print(\\\"   1. Test the model thoroughly with your use cases\\\")\\n\",\n",
    "    \"print(\\\"   2. Fine-tune further if needed\\\")\\n\",\n",
    "    \"print(\\\"   3. Deploy to your preferred platform\\\")\\n\",\n",
    "    \"print(\\\"   4. Monitor performance and user feedback\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üéâ Congratulations! You've successfully trained a code generation model!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"cleanup\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cleanup and finalize\\n\",\n",
    "    \"if wandb.run:\\n\",\n",
    "    \"    wandb.finish()\\n\",\n",
    "    \"    print(\\\"‚úÖ W&B run finished\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Clear GPU memory\\n\",\n",
    "    \"if torch.cuda.is_available():\\n\",\n",
    "    \"    torch.cuda.empty_cache()\\n\",\n",
    "    \"    print(\\\"‚úÖ GPU memory cleared\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüéØ Training Session Complete!\\\")\\n\",\n",
    "    \"print(\\\"Your model has been trained and saved to Google Drive.\\\")\\n\",\n",
    "    \"print(\\\"Feel free to disconnect from Colab to save compute credits.\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"accelerator\": \"GPU\",\n",
    "  \"colab\": {\n",
    "   \"provenance\": [],\n",
    "   \"gpuType\": \"T4\",\n",
    "   \"include_colab_link\": true\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 0\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}