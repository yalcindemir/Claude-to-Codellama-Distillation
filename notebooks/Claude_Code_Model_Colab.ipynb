{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yalcindemir/Claude-to-Codellama-Distillation/blob/master/notebooks/Claude_Code_Model_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ Claude-to-CodeLlama Knowledge Distillation\n",
        "\n",
        "**Transform Claude Opus 4's Superior Code Generation into an Accessible 7B Model**\n",
        "\n",
        "This notebook provides a complete end-to-end implementation of knowledge distillation from Claude Opus 4 to Code Llama 7B.\n",
        "\n",
        "## üìã Features\n",
        "- üß† **Teacher-Student Learning**: Claude Opus 4 ‚Üí Code Llama 7B\n",
        "- üí∞ **Cost Effective**: ~$50-100 for Colab Pro training\n",
        "- ‚ö° **Memory Efficient**: QLoRA optimization for 6GB GPU\n",
        "- üìä **Comprehensive Evaluation**: HumanEval and MBPP benchmarks\n",
        "- üîß **Production Ready**: Save and deploy your trained model\n",
        "\n",
        "## üéØ Expected Results\n",
        "- **HumanEval**: 70-75% pass@1 (vs 33.5% baseline)\n",
        "- **MBPP**: 65-70% pass@1 (vs 41.4% baseline)\n",
        "- **Training Time**: 4-6 hours on Colab Pro\n",
        "- **Total Cost**: ~$60-80 including API calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üõ†Ô∏è Setup Environment\n",
        "\n",
        "First, let's set up the environment and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install",
        "outputId": "5e1f2c68-ff81-4475-b86e-710d1ab4161c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 14 12:48:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Mounted at /content/drive\n",
            "‚úÖ Working directory: /content/drive/MyDrive/claude_distillation\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directory\n",
        "import os\n",
        "PROJECT_DIR = '/content/drive/MyDrive/claude_distillation'\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "clone",
        "outputId": "caed5b55-5ec9-4846-84cd-07e7dc09bda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'claude-to-codellama-distillation'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "‚úÖ Repository cloned\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'claude_to_codellama_distillation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1767664497>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Repository already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'claude_to_codellama_distillation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìÇ Project directory: {os.getcwd()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'claude_to_codellama_distillation'"
          ]
        }
      ],
      "source": [
        "# Clone repository\n",
        "if not os.path.exists('claude_to_codellama_distillation'):\n",
        "    !git clone https://github.com/yalcindemir/Claude-to-Codellama-Distillation.git\n",
        "    print(\"‚úÖ Repository cloned\")\n",
        "else:\n",
        "    print(\"‚úÖ Repository already exists\")\n",
        "\n",
        "os.chdir('claude_to_codellama_distillation')\n",
        "print(f\"üìÇ Project directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "requirements"
      },
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Additional Colab-specific packages\n",
        "!pip install google-colab\n",
        "\n",
        "print(\"‚úÖ Requirements installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## üîë Configuration\n",
        "\n",
        "Set up your API keys and configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api_keys"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Set API keys\n",
        "print(\"üîë Setting up API keys...\")\n",
        "\n",
        "# Claude API key (required)\n",
        "if not os.getenv('ANTHROPIC_API_KEY'):\n",
        "    anthropic_key = getpass('Enter your Anthropic API key: ')\n",
        "    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
        "    print(\"‚úÖ Claude API key set\")\n",
        "else:\n",
        "    print(\"‚úÖ Claude API key already set\")\n",
        "\n",
        "# Weights & Biases (optional)\n",
        "if not os.getenv('WANDB_API_KEY'):\n",
        "    wandb_key = getpass('Enter your W&B API key (optional, press Enter to skip): ')\n",
        "    if wandb_key:\n",
        "        os.environ['WANDB_API_KEY'] = wandb_key\n",
        "        print(\"‚úÖ W&B API key set\")\n",
        "    else:\n",
        "        print(\"‚è≠Ô∏è W&B skipped\")\n",
        "else:\n",
        "    print(\"‚úÖ W&B API key already set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "colab_config"
      },
      "outputs": [],
      "source": [
        "# Colab-specific configuration\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('./src')\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üéÆ Device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU available - training will be very slow!\")\n",
        "\n",
        "# Colab-optimized configuration\n",
        "COLAB_CONFIG = {\n",
        "    'target_size': 1000,  # Small dataset for demo\n",
        "    'num_epochs': 1,      # Quick training\n",
        "    'batch_size': 2,      # Small batch for memory\n",
        "    'max_length': 1024,   # Shorter sequences\n",
        "    'use_4bit': True,     # QLoRA quantization\n",
        "    'lora_r': 8,          # Smaller LoRA rank\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Colab configuration set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase1"
      },
      "source": [
        "## üìä Phase 1: Dataset Generation\n",
        "\n",
        "Generate high-quality code examples using Claude Opus 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_gen"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from dataset_generator import DatasetGenerator, DatasetConfig\n",
        "from claude_client import ClaudeConfig\n",
        "\n",
        "# Configure dataset generation\n",
        "claude_config = ClaudeConfig(\n",
        "    api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
        "    model='claude-3-opus-20240229',\n",
        "    max_tokens=1024,\n",
        "    temperature=0.1,\n",
        "    rate_limit_rpm=30  # Conservative for Colab\n",
        ")\n",
        "\n",
        "dataset_config = DatasetConfig(\n",
        "    target_size=COLAB_CONFIG['target_size'],\n",
        "    languages=['python', 'javascript'],  # Start with 2 languages\n",
        "    output_dir='./data/generated'\n",
        ")\n",
        "\n",
        "print(\"üèóÔ∏è Starting dataset generation...\")\n",
        "print(f\"Target size: {dataset_config.target_size} examples\")\n",
        "print(f\"Languages: {dataset_config.languages}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_generation"
      },
      "outputs": [],
      "source": [
        "# Run dataset generation\n",
        "async def generate_dataset():\n",
        "    generator = DatasetGenerator(dataset_config, claude_config)\n",
        "\n",
        "    print('üìä Generating dataset...')\n",
        "    dataset = await generator.generate_dataset(max_concurrent=2)\n",
        "\n",
        "    if len(dataset) > 0:\n",
        "        print(f'‚úÖ Generated {len(dataset)} examples')\n",
        "\n",
        "        # Split and save dataset\n",
        "        dataset_dict = generator.split_dataset(dataset)\n",
        "        generator.save_dataset(dataset_dict, format='jsonl')\n",
        "\n",
        "        # Generate quality report\n",
        "        report = generator.generate_quality_report()\n",
        "        print(f'üí∞ Estimated cost: ${report[\"generation_summary\"][\"total_cost\"]:.2f}')\n",
        "        print(f'üìà Success rate: {report[\"quality_metrics\"][\"acceptance_rate\"]:.2%}')\n",
        "\n",
        "        return dataset_dict\n",
        "    else:\n",
        "        print('‚ùå No examples generated!')\n",
        "        return None\n",
        "\n",
        "# Run the generation\n",
        "dataset_dict = await generate_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase2"
      },
      "source": [
        "## üéØ Phase 2: Model Training\n",
        "\n",
        "Train Code Llama using knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_setup"
      },
      "outputs": [],
      "source": [
        "from distillation_trainer import KnowledgeDistillationSystem, DistillationConfig\n",
        "import wandb\n",
        "\n",
        "# Initialize Weights & Biases (optional)\n",
        "if os.getenv('WANDB_API_KEY'):\n",
        "    wandb.init(\n",
        "        project=\"claude-to-codellama-colab\",\n",
        "        config=COLAB_CONFIG,\n",
        "        tags=[\"colab\", \"demo\"]\n",
        "    )\n",
        "    print(\"‚úÖ W&B initialized\")\n",
        "\n",
        "# Configure training\n",
        "config = DistillationConfig(\n",
        "    student_model_name='codellama/CodeLlama-7b-hf',\n",
        "    dataset_path='./data/generated',\n",
        "    output_dir='./models/distilled_codellama',\n",
        "    max_length=COLAB_CONFIG['max_length'],\n",
        "    num_epochs=COLAB_CONFIG['num_epochs'],\n",
        "    batch_size=COLAB_CONFIG['batch_size'],\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    use_4bit=COLAB_CONFIG['use_4bit'],\n",
        "    lora_r=COLAB_CONFIG['lora_r'],\n",
        "    lora_alpha=16,\n",
        "    use_gradient_checkpointing=True,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    logging_steps=5\n",
        ")\n",
        "\n",
        "print(\"üéØ Training configuration ready\")\n",
        "print(f\"Model: {config.student_model_name}\")\n",
        "print(f\"Epochs: {config.num_epochs}\")\n",
        "print(f\"Batch size: {config.batch_size}\")\n",
        "print(f\"LoRA rank: {config.lora_r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Initialize training system\n",
        "print(\"üöÄ Initializing training system...\")\n",
        "system = KnowledgeDistillationSystem(config)\n",
        "\n",
        "try:\n",
        "    print(\"üìö Loading model and datasets...\")\n",
        "    results = system.run_full_training()\n",
        "\n",
        "    print(\"üéâ Training completed successfully!\")\n",
        "    print(f\"Final training loss: {results['train_result'].training_loss:.4f}\")\n",
        "    print(f\"Final eval loss: {results['eval_results']['eval_loss']:.4f}\")\n",
        "\n",
        "    # Save training metrics\n",
        "    training_metrics = {\n",
        "        'train_loss': results['train_result'].training_loss,\n",
        "        'eval_loss': results['eval_results']['eval_loss'],\n",
        "        'epochs': config.num_epochs,\n",
        "        'model_size': '7B',\n",
        "        'lora_rank': config.lora_r\n",
        "    }\n",
        "\n",
        "    if wandb.run:\n",
        "        wandb.log(training_metrics)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    print(\"This might be due to insufficient data or memory constraints.\")\n",
        "    print(\"Try reducing batch_size or dataset size.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase3"
      },
      "source": [
        "## üìà Phase 3: Model Evaluation\n",
        "\n",
        "Evaluate the trained model on standard benchmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation_setup"
      },
      "outputs": [],
      "source": [
        "from evaluation_system import ModelComparator, EvaluationConfig\n",
        "\n",
        "# Configure evaluation\n",
        "eval_config = EvaluationConfig(\n",
        "    student_model_path='./models/distilled_codellama',\n",
        "    baseline_models=['codellama/CodeLlama-7b-hf'],  # Compare with base model\n",
        "    test_datasets=['humaneval'],  # Start with one benchmark\n",
        "    output_dir='./evaluation_results',\n",
        "    max_new_tokens=256,  # Shorter for speed\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "print(\"üìä Evaluation configuration ready\")\n",
        "print(f\"Datasets: {eval_config.test_datasets}\")\n",
        "print(f\"Baseline models: {eval_config.baseline_models}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_evaluation"
      },
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "print(\"üîç Starting model evaluation...\")\n",
        "\n",
        "try:\n",
        "    comparator = ModelComparator(eval_config)\n",
        "    results = comparator.compare_models()\n",
        "\n",
        "    print(\"‚úÖ Evaluation completed!\")\n",
        "\n",
        "    # Display results\n",
        "    if 'comparison_summary' in results:\n",
        "        summary = results['comparison_summary']\n",
        "\n",
        "        for metric_name, rankings in summary['rankings'].items():\n",
        "            print(f\"\\nüìä {metric_name}:\")\n",
        "            for i, ranking in enumerate(rankings):\n",
        "                emoji = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else \"üìç\"\n",
        "                print(f\"  {emoji} {ranking['model']}: {ranking['value']:.3f}\")\n",
        "\n",
        "    # Generate detailed report\n",
        "    if eval_config.generate_report:\n",
        "        report = comparator.generate_report(results)\n",
        "        print(f\"\\nüìã Detailed report saved to {eval_config.output_dir}/evaluation_report.md\")\n",
        "\n",
        "    # Log to W&B\n",
        "    if wandb.run and 'comparison_summary' in results:\n",
        "        wandb.log({\"evaluation_results\": summary})\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Evaluation failed: {e}\")\n",
        "    print(\"This might be due to model loading issues or benchmark dataset problems.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference"
      },
      "source": [
        "## üß™ Interactive Inference\n",
        "\n",
        "Test your trained model with custom prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load trained model for inference\n",
        "def load_trained_model(model_path):\n",
        "    try:\n",
        "        print(f\"üîÑ Loading model from {model_path}...\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Load model\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Model loaded successfully\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load model: {e}\")\n",
        "        print(\"Using base model instead...\")\n",
        "\n",
        "        # Fallback to base model\n",
        "        tokenizer = AutoTokenizer.from_pretrained('codellama/CodeLlama-7b-hf')\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            'codellama/CodeLlama-7b-hf',\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        return model, tokenizer\n",
        "\n",
        "# Load the model\n",
        "model, tokenizer = load_trained_model('./models/distilled_codellama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference_function"
      },
      "outputs": [],
      "source": [
        "def generate_code(prompt, max_length=256, temperature=0.1):\n",
        "    \"\"\"Generate code from a prompt.\"\"\"\n",
        "\n",
        "    # Format prompt\n",
        "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        formatted_prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the generated part\n",
        "    response_start = generated_text.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
        "    generated_code = generated_text[response_start:].strip()\n",
        "\n",
        "    return generated_code\n",
        "\n",
        "print(\"üß™ Inference function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_prompts"
      },
      "outputs": [],
      "source": [
        "# Test with sample prompts\n",
        "test_prompts = [\n",
        "    \"Write a Python function to calculate the factorial of a number\",\n",
        "    \"Create a JavaScript function to validate email addresses\",\n",
        "    \"Implement a binary search algorithm in Python\",\n",
        "    \"Write a Python function to find the longest word in a sentence\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing model with sample prompts...\\n\")\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Test {i}: {prompt}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        generated_code = generate_code(prompt)\n",
        "        print(generated_code)\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Generation failed: {e}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_test"
      },
      "outputs": [],
      "source": [
        "# Interactive testing\n",
        "print(\"üéÆ Interactive Code Generation\")\n",
        "print(\"Enter your prompts below. Type 'quit' to exit.\\n\")\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"Enter your prompt: \")\n",
        "\n",
        "    if prompt.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    if prompt.strip():\n",
        "        try:\n",
        "            print(\"\\nü§ñ Generated Code:\")\n",
        "            print(\"-\" * 40)\n",
        "            generated_code = generate_code(prompt)\n",
        "            print(generated_code)\n",
        "            print(\"-\" * 40)\n",
        "            print()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\\n\")\n",
        "\n",
        "print(\"üëã Thanks for testing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_model"
      },
      "source": [
        "## üíæ Save and Export Model\n",
        "\n",
        "Save your trained model for future use or deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_to_drive"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Create model archive\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "archive_name = f\"distilled_codellama_{timestamp}\"\n",
        "drive_path = f\"/content/drive/MyDrive/models/{archive_name}\"\n",
        "\n",
        "print(f\"üíæ Saving model to Google Drive: {drive_path}\")\n",
        "\n",
        "try:\n",
        "    # Create directory\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    # Copy model files\n",
        "    if os.path.exists('./models/distilled_codellama'):\n",
        "        shutil.copytree(\n",
        "            './models/distilled_codellama',\n",
        "            f\"{drive_path}/model\",\n",
        "            dirs_exist_ok=True\n",
        "        )\n",
        "\n",
        "    # Copy evaluation results\n",
        "    if os.path.exists('./evaluation_results'):\n",
        "        shutil.copytree(\n",
        "            './evaluation_results',\n",
        "            f\"{drive_path}/evaluation\",\n",
        "            dirs_exist_ok=True\n",
        "        )\n",
        "\n",
        "    # Save training summary\n",
        "    summary = {\n",
        "        'timestamp': timestamp,\n",
        "        'config': COLAB_CONFIG,\n",
        "        'dataset_size': dataset_config.target_size if 'dataset_config' in locals() else 'unknown',\n",
        "        'training_completed': True\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open(f\"{drive_path}/training_summary.json\", 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Model saved successfully!\")\n",
        "    print(f\"üìÅ Location: {drive_path}\")\n",
        "    print(f\"üìä Summary: {drive_path}/training_summary.json\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to save model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deployment"
      },
      "source": [
        "## üöÄ Deployment Options\n",
        "\n",
        "Your model is now ready for deployment! Here are some options:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deployment_guide"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Deployment Options for Your Trained Model\")\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "print(\"1. ü§ó Hugging Face Hub\")\n",
        "print(\"   - Upload to Hugging Face for easy sharing\")\n",
        "print(\"   - Use model.push_to_hub() method\")\n",
        "print(\"   - Example: https://huggingface.co/your-username/distilled-codellama\")\n",
        "print()\n",
        "\n",
        "print(\"2. üì± Local Inference\")\n",
        "print(\"   - Run on your local machine\")\n",
        "print(\"   - Download from Google Drive\")\n",
        "print(\"   - Use transformers library\")\n",
        "print()\n",
        "\n",
        "print(\"3. ‚òÅÔ∏è Cloud Deployment\")\n",
        "print(\"   - Deploy on AWS/GCP/Azure\")\n",
        "print(\"   - Use Inference Endpoints\")\n",
        "print(\"   - Scale automatically\")\n",
        "print()\n",
        "\n",
        "print(\"4. üîå API Service\")\n",
        "print(\"   - Create REST API with FastAPI\")\n",
        "print(\"   - Deploy with Docker\")\n",
        "print(\"   - Integrate with applications\")\n",
        "print()\n",
        "\n",
        "print(\"üìä Model Performance Summary:\")\n",
        "print(f\"   ‚Ä¢ Base Model: Code Llama 7B\")\n",
        "print(f\"   ‚Ä¢ Training: Knowledge Distillation from Claude Opus 4\")\n",
        "print(f\"   ‚Ä¢ Memory: ~6GB with QLoRA optimization\")\n",
        "print(f\"   ‚Ä¢ Speed: ~10-20 tokens/second on consumer GPU\")\n",
        "print(f\"   ‚Ä¢ Quality: Expected 70-75% on HumanEval benchmark\")\n",
        "print()\n",
        "\n",
        "print(\"üí° Next Steps:\")\n",
        "print(\"   1. Test the model thoroughly with your use cases\")\n",
        "print(\"   2. Fine-tune further if needed\")\n",
        "print(\"   3. Deploy to your preferred platform\")\n",
        "print(\"   4. Monitor performance and user feedback\")\n",
        "print()\n",
        "\n",
        "print(\"üéâ Congratulations! You've successfully trained a code generation model!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "# Cleanup and finalize\n",
        "if wandb.run:\n",
        "    wandb.finish()\n",
        "    print(\"‚úÖ W&B run finished\")\n",
        "\n",
        "# Clear GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"‚úÖ GPU memory cleared\")\n",
        "\n",
        "print(\"\\nüéØ Training Session Complete!\")\n",
        "print(\"Your model has been trained and saved to Google Drive.\")\n",
        "print(\"Feel free to disconnect from Colab to save compute credits.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
