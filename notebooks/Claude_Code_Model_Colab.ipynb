{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yalcindemir/Claude-to-Codellama-Distillation/blob/master/notebooks/Claude_Code_Model_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ Claude-to-CodeLlama Knowledge Distillation\n",
        "\n",
        "**Transform Claude Opus 4's Superior Code Generation into an Accessible 7B Model**\n",
        "\n",
        "This notebook provides a complete end-to-end implementation of knowledge distillation from Claude Opus 4 to Code Llama 7B.\n",
        "\n",
        "## üìã Features\n",
        "- üß† **Teacher-Student Learning**: Claude Opus 4 ‚Üí Code Llama 7B\n",
        "- üí∞ **Cost Effective**: ~$50-100 for Colab Pro training\n",
        "- ‚ö° **Memory Efficient**: QLoRA optimization for 6GB GPU\n",
        "- üìä **Comprehensive Evaluation**: HumanEval and MBPP benchmarks\n",
        "- üîß **Production Ready**: Save and deploy your trained model\n",
        "\n",
        "## üéØ Expected Results\n",
        "- **HumanEval**: 70-75% pass@1 (vs 33.5% baseline)\n",
        "- **MBPP**: 65-70% pass@1 (vs 41.4% baseline)\n",
        "- **Training Time**: 4-6 hours on Colab Pro\n",
        "- **Total Cost**: ~$60-80 including API calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üõ†Ô∏è Setup Environment\n",
        "\n",
        "First, let's set up the environment and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "install",
        "outputId": "7b10ccf4-bb2c-4130-9882-b9eba2e14baf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 14 14:12:43 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Working directory: /content/drive/MyDrive/claude_distillation\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directory\n",
        "import os\n",
        "PROJECT_DIR = '/content/drive/MyDrive/claude_distillation'\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "print(f\"‚úÖ Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "clone",
        "outputId": "d75330b7-cd80-49ad-b08a-d8c0eb7f2058",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Repository already exists\n"
          ]
        }
      ],
      "source": [
        "# Clone repository\n",
        "if not os.path.exists('Claude-to-Codellama-Distillation'):\n",
        "    !git clone https://github.com/yalcindemir/Claude-to-Codellama-Distillation.git\n",
        "    print(\"‚úÖ Repository cloned\")\n",
        "else:\n",
        "    print(\"‚úÖ Repository already exists\")\n",
        "\n",
        "# Removed os.chdir as the directory was not created due to the git clone failure.\n",
        "# You will need to address the git clone error before proceeding.\n",
        "# print(f\"üìÇ Project directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "requirements",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9264e6-3b48-4849-95ed-93bf8b2f0092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: google-colab in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: google-auth==2.38.0 in /usr/local/lib/python3.11/dist-packages (from google-colab) (2.38.0)\n",
            "Requirement already satisfied: ipykernel==6.17.1 in /usr/local/lib/python3.11/dist-packages (from google-colab) (6.17.1)\n",
            "Requirement already satisfied: ipyparallel==8.8.0 in /usr/local/lib/python3.11/dist-packages (from google-colab) (8.8.0)\n",
            "Requirement already satisfied: ipython==7.34.0 in /usr/local/lib/python3.11/dist-packages (from google-colab) (7.34.0)\n",
            "Requirement already satisfied: notebook==6.5.7 in /usr/local/lib/python3.11/dist-packages (from google-colab) (6.5.7)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from google-colab) (2.2.2)\n",
            "Requirement already satisfied: portpicker==1.5.2 in /usr/local/lib/python3.11/dist-packages (from google-colab) (1.5.2)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from google-colab) (2.32.3)\n",
            "Requirement already satisfied: tornado==6.4.2 in /usr/local/lib/python3.11/dist-packages (from google-colab) (6.4.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.38.0->google-colab) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.38.0->google-colab) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth==2.38.0->google-colab) (4.9.1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (24.0.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel==6.17.1->google-colab) (5.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (4.4.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ipyparallel==8.8.0->google-colab) (4.67.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (0.19.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython==7.34.0->google-colab) (4.9.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (25.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (5.8.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook==6.5.7->google-colab) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->google-colab) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->google-colab) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->google-colab) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google-colab) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google-colab) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google-colab) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3->google-colab) (2025.4.26)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython==7.34.0->google-colab) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook==6.5.7->google-colab) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook==6.5.7->google-colab) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.7->google-colab) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook==6.5.7->google-colab) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook==6.5.7->google-colab) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook==6.5.7->google-colab) (4.24.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython==7.34.0->google-colab) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-colab) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth==2.38.0->google-colab) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->ipyparallel==8.8.0->google-colab) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook==6.5.7->google-colab) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.7->google-colab) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook==6.5.7->google-colab) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.7->google-colab) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.7->google-colab) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.7->google-colab) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook==6.5.7->google-colab) (0.25.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.7->google-colab) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook==6.5.7->google-colab) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.7->google-colab) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook==6.5.7->google-colab) (4.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook==6.5.7->google-colab) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.7->google-colab) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.7->google-colab) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook==6.5.7->google-colab) (1.3.1)\n",
            "‚úÖ Requirements installed\n"
          ]
        }
      ],
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Additional Colab-specific packages\n",
        "!pip install google-colab\n",
        "\n",
        "print(\"‚úÖ Requirements installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## üîë Configuration\n",
        "\n",
        "Set up your API keys and configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "api_keys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37b814a-6ed0-4a07-f010-31d0dc091c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Setting up API keys...\n",
            "‚úÖ Claude API key already set\n",
            "Enter your W&B API key (optional, press Enter to skip): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ W&B API key set\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Set API keys\n",
        "print(\"üîë Setting up API keys...\")\n",
        "\n",
        "# Claude API key (required)\n",
        "if not os.getenv('ANTHROPIC_API_KEY'):\n",
        "    anthropic_key = getpass('Enter your Anthropic API key: ')\n",
        "    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
        "    print(\"‚úÖ Claude API key set\")\n",
        "else:\n",
        "    print(\"‚úÖ Claude API key already set\")\n",
        "\n",
        "# Weights & Biases (optional)\n",
        "if not os.getenv('WANDB_API_KEY'):\n",
        "    wandb_key = getpass('Enter your W&B API key (optional, press Enter to skip): ')\n",
        "    if wandb_key:\n",
        "        os.environ['WANDB_API_KEY'] = wandb_key\n",
        "        print(\"‚úÖ W&B API key set\")\n",
        "    else:\n",
        "        print(\"‚è≠Ô∏è W&B skipped\")\n",
        "else:\n",
        "    print(\"‚úÖ W&B API key already set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "colab_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0c886c-bac2-402f-b4fc-efd3d22c3839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéÆ Device: cuda\n",
            "GPU: Tesla T4 (15.8GB)\n",
            "‚úÖ Colab configuration set\n"
          ]
        }
      ],
      "source": [
        "# Colab-specific configuration\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('./src')\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üéÆ Device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU available - training will be very slow!\")\n",
        "\n",
        "# Colab-optimized configuration\n",
        "COLAB_CONFIG = {\n",
        "    'target_size': 1000,  # Small dataset for demo\n",
        "    'num_epochs': 1,      # Quick training\n",
        "    'batch_size': 2,      # Small batch for memory\n",
        "    'max_length': 1024,   # Shorter sequences\n",
        "    'use_4bit': True,     # QLoRA quantization\n",
        "    'lora_r': 8,          # Smaller LoRA rank\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Colab configuration set\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Doƒüru dizine ge√ßin\n",
        "os.chdir('/content/drive/MyDrive/claude_distillation/Claude-to-Codellama-Distillation')\n",
        "print(f\"‚úÖ Yeni dizin: {os.getcwd()}\")\n",
        "\n",
        "# ≈ûimdi src klas√∂r√ºn√º kontrol edin\n",
        "if os.path.exists('./src'):\n",
        "    print(\"‚úÖ ./src klas√∂r√º bulundu\")\n",
        "else:\n",
        "    print(\"‚ùå src klas√∂r√º hala yok\")\n",
        "    # Dosya listesini g√∂relim\n",
        "    print(\"Mevcut dosyalar:\")\n",
        "    print(os.listdir('.'))"
      ],
      "metadata": {
        "id": "-eLrAq-Iw7V4",
        "outputId": "3fe784bf-f010-47c8-abe7-443850408d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/claude_distillation/claude_to_codellama_distillation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-695744257>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Doƒüru dizine ge√ßin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/claude_distillation/claude_to_codellama_distillation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ Yeni dizin: {os.getcwd()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/claude_distillation/claude_to_codellama_distillation'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase1"
      },
      "source": [
        "## üìä Phase 1: Dataset Generation\n",
        "\n",
        "Generate high-quality code examples using Claude Opus 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_gen"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from dataset_generator import DatasetGenerator, DatasetConfig\n",
        "from claude_client import ClaudeConfig\n",
        "\n",
        "# Configure dataset generation\n",
        "claude_config = ClaudeConfig(\n",
        "    api_key=os.getenv('ANTHROPIC_API_KEY'),\n",
        "    model='claude-3-opus-20240229',\n",
        "    max_tokens=1024,\n",
        "    temperature=0.1,\n",
        "    rate_limit_rpm=30  # Conservative for Colab\n",
        ")\n",
        "\n",
        "dataset_config = DatasetConfig(\n",
        "    target_size=COLAB_CONFIG['target_size'],\n",
        "    languages=['python', 'javascript'],  # Start with 2 languages\n",
        "    output_dir='./data/generated'\n",
        ")\n",
        "\n",
        "print(\"üèóÔ∏è Starting dataset generation...\")\n",
        "print(f\"Target size: {dataset_config.target_size} examples\")\n",
        "print(f\"Languages: {dataset_config.languages}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_generation"
      },
      "outputs": [],
      "source": [
        "# Run dataset generation\n",
        "async def generate_dataset():\n",
        "    generator = DatasetGenerator(dataset_config, claude_config)\n",
        "\n",
        "    print('üìä Generating dataset...')\n",
        "    dataset = await generator.generate_dataset(max_concurrent=2)\n",
        "\n",
        "    if len(dataset) > 0:\n",
        "        print(f'‚úÖ Generated {len(dataset)} examples')\n",
        "\n",
        "        # Split and save dataset\n",
        "        dataset_dict = generator.split_dataset(dataset)\n",
        "        generator.save_dataset(dataset_dict, format='jsonl')\n",
        "\n",
        "        # Generate quality report\n",
        "        report = generator.generate_quality_report()\n",
        "        print(f'üí∞ Estimated cost: ${report[\"generation_summary\"][\"total_cost\"]:.2f}')\n",
        "        print(f'üìà Success rate: {report[\"quality_metrics\"][\"acceptance_rate\"]:.2%}')\n",
        "\n",
        "        return dataset_dict\n",
        "    else:\n",
        "        print('‚ùå No examples generated!')\n",
        "        return None\n",
        "\n",
        "# Run the generation\n",
        "dataset_dict = await generate_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase2"
      },
      "source": [
        "## üéØ Phase 2: Model Training\n",
        "\n",
        "Train Code Llama using knowledge distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_setup"
      },
      "outputs": [],
      "source": [
        "from distillation_trainer import KnowledgeDistillationSystem, DistillationConfig\n",
        "import wandb\n",
        "\n",
        "# Initialize Weights & Biases (optional)\n",
        "if os.getenv('WANDB_API_KEY'):\n",
        "    wandb.init(\n",
        "        project=\"claude-to-codellama-colab\",\n",
        "        config=COLAB_CONFIG,\n",
        "        tags=[\"colab\", \"demo\"]\n",
        "    )\n",
        "    print(\"‚úÖ W&B initialized\")\n",
        "\n",
        "# Configure training\n",
        "config = DistillationConfig(\n",
        "    student_model_name='codellama/CodeLlama-7b-hf',\n",
        "    dataset_path='./data/generated',\n",
        "    output_dir='./models/distilled_codellama',\n",
        "    max_length=COLAB_CONFIG['max_length'],\n",
        "    num_epochs=COLAB_CONFIG['num_epochs'],\n",
        "    batch_size=COLAB_CONFIG['batch_size'],\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    use_4bit=COLAB_CONFIG['use_4bit'],\n",
        "    lora_r=COLAB_CONFIG['lora_r'],\n",
        "    lora_alpha=16,\n",
        "    use_gradient_checkpointing=True,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    logging_steps=5\n",
        ")\n",
        "\n",
        "print(\"üéØ Training configuration ready\")\n",
        "print(f\"Model: {config.student_model_name}\")\n",
        "print(f\"Epochs: {config.num_epochs}\")\n",
        "print(f\"Batch size: {config.batch_size}\")\n",
        "print(f\"LoRA rank: {config.lora_r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Initialize training system\n",
        "print(\"üöÄ Initializing training system...\")\n",
        "system = KnowledgeDistillationSystem(config)\n",
        "\n",
        "try:\n",
        "    print(\"üìö Loading model and datasets...\")\n",
        "    results = system.run_full_training()\n",
        "\n",
        "    print(\"üéâ Training completed successfully!\")\n",
        "    print(f\"Final training loss: {results['train_result'].training_loss:.4f}\")\n",
        "    print(f\"Final eval loss: {results['eval_results']['eval_loss']:.4f}\")\n",
        "\n",
        "    # Save training metrics\n",
        "    training_metrics = {\n",
        "        'train_loss': results['train_result'].training_loss,\n",
        "        'eval_loss': results['eval_results']['eval_loss'],\n",
        "        'epochs': config.num_epochs,\n",
        "        'model_size': '7B',\n",
        "        'lora_rank': config.lora_r\n",
        "    }\n",
        "\n",
        "    if wandb.run:\n",
        "        wandb.log(training_metrics)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Training failed: {e}\")\n",
        "    print(\"This might be due to insufficient data or memory constraints.\")\n",
        "    print(\"Try reducing batch_size or dataset size.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phase3"
      },
      "source": [
        "## üìà Phase 3: Model Evaluation\n",
        "\n",
        "Evaluate the trained model on standard benchmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation_setup"
      },
      "outputs": [],
      "source": [
        "from evaluation_system import ModelComparator, EvaluationConfig\n",
        "\n",
        "# Configure evaluation\n",
        "eval_config = EvaluationConfig(\n",
        "    student_model_path='./models/distilled_codellama',\n",
        "    baseline_models=['codellama/CodeLlama-7b-hf'],  # Compare with base model\n",
        "    test_datasets=['humaneval'],  # Start with one benchmark\n",
        "    output_dir='./evaluation_results',\n",
        "    max_new_tokens=256,  # Shorter for speed\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "print(\"üìä Evaluation configuration ready\")\n",
        "print(f\"Datasets: {eval_config.test_datasets}\")\n",
        "print(f\"Baseline models: {eval_config.baseline_models}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_evaluation"
      },
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "print(\"üîç Starting model evaluation...\")\n",
        "\n",
        "try:\n",
        "    comparator = ModelComparator(eval_config)\n",
        "    results = comparator.compare_models()\n",
        "\n",
        "    print(\"‚úÖ Evaluation completed!\")\n",
        "\n",
        "    # Display results\n",
        "    if 'comparison_summary' in results:\n",
        "        summary = results['comparison_summary']\n",
        "\n",
        "        for metric_name, rankings in summary['rankings'].items():\n",
        "            print(f\"\\nüìä {metric_name}:\")\n",
        "            for i, ranking in enumerate(rankings):\n",
        "                emoji = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else \"üìç\"\n",
        "                print(f\"  {emoji} {ranking['model']}: {ranking['value']:.3f}\")\n",
        "\n",
        "    # Generate detailed report\n",
        "    if eval_config.generate_report:\n",
        "        report = comparator.generate_report(results)\n",
        "        print(f\"\\nüìã Detailed report saved to {eval_config.output_dir}/evaluation_report.md\")\n",
        "\n",
        "    # Log to W&B\n",
        "    if wandb.run and 'comparison_summary' in results:\n",
        "        wandb.log({\"evaluation_results\": summary})\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Evaluation failed: {e}\")\n",
        "    print(\"This might be due to model loading issues or benchmark dataset problems.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference"
      },
      "source": [
        "## üß™ Interactive Inference\n",
        "\n",
        "Test your trained model with custom prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load trained model for inference\n",
        "def load_trained_model(model_path):\n",
        "    try:\n",
        "        print(f\"üîÑ Loading model from {model_path}...\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Load model\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Model loaded successfully\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load model: {e}\")\n",
        "        print(\"Using base model instead...\")\n",
        "\n",
        "        # Fallback to base model\n",
        "        tokenizer = AutoTokenizer.from_pretrained('codellama/CodeLlama-7b-hf')\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            'codellama/CodeLlama-7b-hf',\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        return model, tokenizer\n",
        "\n",
        "# Load the model\n",
        "model, tokenizer = load_trained_model('./models/distilled_codellama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference_function"
      },
      "outputs": [],
      "source": [
        "def generate_code(prompt, max_length=256, temperature=0.1):\n",
        "    \"\"\"Generate code from a prompt.\"\"\"\n",
        "\n",
        "    # Format prompt\n",
        "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        formatted_prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the generated part\n",
        "    response_start = generated_text.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
        "    generated_code = generated_text[response_start:].strip()\n",
        "\n",
        "    return generated_code\n",
        "\n",
        "print(\"üß™ Inference function ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_prompts"
      },
      "outputs": [],
      "source": [
        "# Test with sample prompts\n",
        "test_prompts = [\n",
        "    \"Write a Python function to calculate the factorial of a number\",\n",
        "    \"Create a JavaScript function to validate email addresses\",\n",
        "    \"Implement a binary search algorithm in Python\",\n",
        "    \"Write a Python function to find the longest word in a sentence\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing model with sample prompts...\\n\")\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Test {i}: {prompt}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        generated_code = generate_code(prompt)\n",
        "        print(generated_code)\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Generation failed: {e}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_test"
      },
      "outputs": [],
      "source": [
        "# Interactive testing\n",
        "print(\"üéÆ Interactive Code Generation\")\n",
        "print(\"Enter your prompts below. Type 'quit' to exit.\\n\")\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"Enter your prompt: \")\n",
        "\n",
        "    if prompt.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    if prompt.strip():\n",
        "        try:\n",
        "            print(\"\\nü§ñ Generated Code:\")\n",
        "            print(\"-\" * 40)\n",
        "            generated_code = generate_code(prompt)\n",
        "            print(generated_code)\n",
        "            print(\"-\" * 40)\n",
        "            print()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\\n\")\n",
        "\n",
        "print(\"üëã Thanks for testing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_model"
      },
      "source": [
        "## üíæ Save and Export Model\n",
        "\n",
        "Save your trained model for future use or deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_to_drive"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Create model archive\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "archive_name = f\"distilled_codellama_{timestamp}\"\n",
        "drive_path = f\"/content/drive/MyDrive/models/{archive_name}\"\n",
        "\n",
        "print(f\"üíæ Saving model to Google Drive: {drive_path}\")\n",
        "\n",
        "try:\n",
        "    # Create directory\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    # Copy model files\n",
        "    if os.path.exists('./models/distilled_codellama'):\n",
        "        shutil.copytree(\n",
        "            './models/distilled_codellama',\n",
        "            f\"{drive_path}/model\",\n",
        "            dirs_exist_ok=True\n",
        "        )\n",
        "\n",
        "    # Copy evaluation results\n",
        "    if os.path.exists('./evaluation_results'):\n",
        "        shutil.copytree(\n",
        "            './evaluation_results',\n",
        "            f\"{drive_path}/evaluation\",\n",
        "            dirs_exist_ok=True\n",
        "        )\n",
        "\n",
        "    # Save training summary\n",
        "    summary = {\n",
        "        'timestamp': timestamp,\n",
        "        'config': COLAB_CONFIG,\n",
        "        'dataset_size': dataset_config.target_size if 'dataset_config' in locals() else 'unknown',\n",
        "        'training_completed': True\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open(f\"{drive_path}/training_summary.json\", 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Model saved successfully!\")\n",
        "    print(f\"üìÅ Location: {drive_path}\")\n",
        "    print(f\"üìä Summary: {drive_path}/training_summary.json\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to save model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deployment"
      },
      "source": [
        "## üöÄ Deployment Options\n",
        "\n",
        "Your model is now ready for deployment! Here are some options:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deployment_guide"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Deployment Options for Your Trained Model\")\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "print(\"1. ü§ó Hugging Face Hub\")\n",
        "print(\"   - Upload to Hugging Face for easy sharing\")\n",
        "print(\"   - Use model.push_to_hub() method\")\n",
        "print(\"   - Example: https://huggingface.co/your-username/distilled-codellama\")\n",
        "print()\n",
        "\n",
        "print(\"2. üì± Local Inference\")\n",
        "print(\"   - Run on your local machine\")\n",
        "print(\"   - Download from Google Drive\")\n",
        "print(\"   - Use transformers library\")\n",
        "print()\n",
        "\n",
        "print(\"3. ‚òÅÔ∏è Cloud Deployment\")\n",
        "print(\"   - Deploy on AWS/GCP/Azure\")\n",
        "print(\"   - Use Inference Endpoints\")\n",
        "print(\"   - Scale automatically\")\n",
        "print()\n",
        "\n",
        "print(\"4. üîå API Service\")\n",
        "print(\"   - Create REST API with FastAPI\")\n",
        "print(\"   - Deploy with Docker\")\n",
        "print(\"   - Integrate with applications\")\n",
        "print()\n",
        "\n",
        "print(\"üìä Model Performance Summary:\")\n",
        "print(f\"   ‚Ä¢ Base Model: Code Llama 7B\")\n",
        "print(f\"   ‚Ä¢ Training: Knowledge Distillation from Claude Opus 4\")\n",
        "print(f\"   ‚Ä¢ Memory: ~6GB with QLoRA optimization\")\n",
        "print(f\"   ‚Ä¢ Speed: ~10-20 tokens/second on consumer GPU\")\n",
        "print(f\"   ‚Ä¢ Quality: Expected 70-75% on HumanEval benchmark\")\n",
        "print()\n",
        "\n",
        "print(\"üí° Next Steps:\")\n",
        "print(\"   1. Test the model thoroughly with your use cases\")\n",
        "print(\"   2. Fine-tune further if needed\")\n",
        "print(\"   3. Deploy to your preferred platform\")\n",
        "print(\"   4. Monitor performance and user feedback\")\n",
        "print()\n",
        "\n",
        "print(\"üéâ Congratulations! You've successfully trained a code generation model!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "# Cleanup and finalize\n",
        "if wandb.run:\n",
        "    wandb.finish()\n",
        "    print(\"‚úÖ W&B run finished\")\n",
        "\n",
        "# Clear GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"‚úÖ GPU memory cleared\")\n",
        "\n",
        "print(\"\\nüéØ Training Session Complete!\")\n",
        "print(\"Your model has been trained and saved to Google Drive.\")\n",
        "print(\"Feel free to disconnect from Colab to save compute credits.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "lWf2b27j_3e9",
        "outputId": "4551e5d7-cc70-451f-d332-67b0d4847659"
      },
      "source": [
        "os.chdir('Claude-to-Codellama-Distillation')\n",
        "print(f\"üìÇ Project directory: {os.getcwd()}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Claude-to-Codellama-Distillation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-547505666>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Claude-to-Codellama-Distillation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìÇ Project directory: {os.getcwd()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Claude-to-Codellama-Distillation'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}