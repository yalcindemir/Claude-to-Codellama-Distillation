{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yalcindemir/Claude-to-Codellama-Distillation/blob/main/notebooks/Claude_Code_Model_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Claude-to-CodeLlama Knowledge Distillation\n",
    "\n",
    "**Transform Claude Opus 4's Superior Code Generation into an Accessible 7B Model**\n",
    "\n",
    "This notebook provides a complete end-to-end implementation of knowledge distillation from Claude Opus 4 to Code Llama 7B.\n",
    "\n",
    "## üìã Features\n",
    "- üß† **Teacher-Student Learning**: Claude Opus 4 ‚Üí Code Llama 7B\n",
    "- üí∞ **Cost Effective**: ~$50-100 for Colab Pro training\n",
    "- ‚ö° **Memory Efficient**: QLoRA optimization for 6GB GPU\n",
    "- üìä **Comprehensive Evaluation**: HumanEval and MBPP benchmarks\n",
    "- üîß **Production Ready**: Save and deploy your trained model\n",
    "\n",
    "## üéØ Expected Results\n",
    "- **HumanEval**: 70-75% pass@1 (vs 33.5% baseline)\n",
    "- **MBPP**: 65-70% pass@1 (vs 41.4% baseline)\n",
    "- **Training Time**: 4-6 hours on Colab Pro\n",
    "- **Total Cost**: ~$60-80 including API calls"
   ]
  },
  {
   "cell_type": "code",
   "source": "# üöÄ Quick Dependency Installation\n# Run this cell first if you encounter import errors\n\nprint(\"üîß Installing critical dependencies for Colab...\")\n\n# Core dependencies that are commonly missing\ncritical_deps = [\n    \"anthropic>=0.25.0\",\n    \"backoff>=2.2.1\", \n    \"pyyaml>=6.0\",\n    \"tqdm>=4.65.0\",\n    \"datasets>=2.14.0\",\n    \"bitsandbytes>=0.41.0\",\n    \"transformers>=4.35.0\",\n    \"accelerate>=0.24.0\",\n    \"peft>=0.6.0\"\n]\n\nfor dep in critical_deps:\n    try:\n        !pip install -q {dep}\n        print(f\"‚úÖ {dep}\")\n    except Exception as e:\n        print(f\"‚ùå Failed to install {dep}: {e}\")\n\nprint(\"üéØ Critical dependencies installation complete!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üõ†Ô∏è Setup Environment\n",
    "\n",
    "First, let's set up the environment and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install",
    "outputId": "7b10ccf4-bb2c-4130-9882-b9eba2e14baf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Install essential dependencies immediately\nprint(\"üîß Installing essential dependencies...\")\n!pip install -q anthropic backoff pyyaml tqdm datasets bitsandbytes\n\n# Clone repository with correct URL\nif not os.path.exists('Claude-to-Codellama-Distillation'):\n    !git clone https://github.com/yalcindemir/Claude-to-Codellama-Distillation.git\n    print(\"‚úÖ Repository cloned\")\nelse:\n    print(\"‚úÖ Repository already exists\")\n\n# Change to the correct directory\nos.chdir('Claude-to-Codellama-Distillation')\nprint(f\"üìÇ Project directory: {os.getcwd()}\")\n\n# Install the package in editable mode to fix imports\ntry:\n    # Try editable install first\n    result = !pip install -e . 2>&1\n    if \"error\" in ' '.join(result).lower():\n        print(\"‚ö†Ô∏è Editable install failed, trying manual path setup...\")\n        raise Exception(\"Setup failed\")\n    else:\n        print(\"‚úÖ Package installed successfully\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Package installation failed: {e}\")\n    print(\"üìÅ Using manual path setup instead...\")\n    \n    # Manual path setup as fallback\n    import sys\n    import os\n    \n    # Add src directory to Python path\n    src_path = os.path.join(os.getcwd(), 'src')\n    if src_path not in sys.path:\n        sys.path.insert(0, src_path)\n    \n    # Verify the path setup\n    if os.path.exists('./src'):\n        print(\"‚úÖ Manual path setup completed\")\n        print(f\"üìÅ src directory found: {os.path.exists('./src')}\")\n        src_files = [f for f in os.listdir('./src') if f.endswith('.py')]\n        print(f\"üìÑ Python files in src: {src_files}\")\n    else:\n        print(\"‚ùå src directory not found\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "clone",
    "outputId": "d75330b7-cd80-49ad-b08a-d8c0eb7f2058",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# Install essential dependencies first\n!pip install anthropic>=0.25.0\n!pip install backoff>=2.2.1\n!pip install pyyaml>=6.0\n!pip install tqdm>=4.65.0\n!pip install datasets>=2.14.0\n!pip install bitsandbytes>=0.41.0\n\n# Install core ML dependencies\n!pip install torch>=2.0.0\n!pip install transformers>=4.35.0\n!pip install accelerate>=0.24.0\n!pip install peft>=0.6.0\n\n# Install evaluation dependencies\n!pip install evaluate>=0.4.0\n!pip install scikit-learn>=1.3.0\n\n# Try to install from Colab-specific requirements if available\nimport os\nif os.path.exists('requirements-colab.txt'):\n    !pip install -r requirements-colab.txt\n    print(\"‚úÖ Colab requirements installed\")\nelif os.path.exists('requirements.txt'):\n    # Install core packages from requirements.txt, skip problematic ones\n    !pip install -r requirements.txt --no-deps --force-reinstall anthropic backoff pyyaml tqdm datasets transformers\n    print(\"‚úÖ Core requirements from file installed\")\nelse:\n    print(\"‚ÑπÔ∏è No requirements file found, using manual installation\")\n\n# Additional Colab-specific packages\n!pip install google-colab wandb\n\nprint(\"‚úÖ All dependencies installed successfully\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "requirements",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ad9264e6-3b48-4849-95ed-93bf8b2f0092"
   },
   "outputs": [],
   "source": "# Install essential dependencies first\n!pip install anthropic>=0.25.0\n!pip install backoff>=2.2.1\n!pip install pyyaml>=6.0\n!pip install tqdm>=4.65.0\n!pip install datasets>=2.14.0\n!pip install bitsandbytes>=0.41.0\n\n# Install core ML dependencies\n!pip install torch>=2.0.0\n!pip install transformers>=4.35.0\n!pip install accelerate>=0.24.0\n!pip install peft>=0.6.0\n\n# Install evaluation dependencies\n!pip install evaluate>=0.4.0\n!pip install scikit-learn>=1.3.0\n\n# Install requirements from file (if exists)\nimport os\nif os.path.exists('requirements.txt'):\n    !pip install -r requirements.txt\n    print(\"‚úÖ Requirements from file installed\")\nelse:\n    print(\"‚ÑπÔ∏è No requirements.txt found, using manual installation\")\n\n# Additional Colab-specific packages\n!pip install google-colab\n\nprint(\"‚úÖ All dependencies installed successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## üîë Configuration\n",
    "\n",
    "Set up your API keys and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "api_keys",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d37b814a-6ed0-4a07-f010-31d0dc091c1d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üîë Setting up API keys...\n",
      "‚úÖ Claude API key already set\n",
      "Enter your W&B API key (optional, press Enter to skip): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "‚úÖ W&B API key set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API keys\n",
    "print(\"üîë Setting up API keys...\")\n",
    "\n",
    "# Claude API key (required)\n",
    "if not os.getenv('ANTHROPIC_API_KEY'):\n",
    "    anthropic_key = getpass('Enter your Anthropic API key: ')\n",
    "    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
    "    print(\"‚úÖ Claude API key set\")\n",
    "else:\n",
    "    print(\"‚úÖ Claude API key already set\")\n",
    "\n",
    "# Weights & Biases (optional)\n",
    "if not os.getenv('WANDB_API_KEY'):\n",
    "    wandb_key = getpass('Enter your W&B API key (optional, press Enter to skip): ')\n",
    "    if wandb_key:\n",
    "        os.environ['WANDB_API_KEY'] = wandb_key\n",
    "        print(\"‚úÖ W&B API key set\")\n",
    "    else:\n",
    "        print(\"‚è≠Ô∏è W&B skipped\")\n",
    "else:\n",
    "    print(\"‚úÖ W&B API key already set\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "colab_config",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eb0c886c-bac2-402f-b4fc-efd3d22c3839"
   },
   "outputs": [],
   "source": "# Colab-specific configuration\nimport torch\nimport sys\n\n# Add src to path (multiple ways for reliability)\nsys.path.append('./src')\nsys.path.append('/content/drive/MyDrive/claude_distillation/Claude-to-Codellama-Distillation/src')\n\n# Verify we're in the right directory\nimport os\nprint(f\"üìÇ Current directory: {os.getcwd()}\")\nif os.path.exists('./src'):\n    print(\"‚úÖ src directory found\")\n    # List Python files in src\n    src_files = [f for f in os.listdir('./src') if f.endswith('.py')]\n    print(f\"üìÑ Python files in src: {src_files}\")\nelse:\n    print(\"‚ùå src directory not found - need to navigate to project directory first\")\n\n# Check GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üéÆ Device: {device}\")\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\nelse:\n    print(\"‚ö†Ô∏è No GPU available - training will be very slow!\")\n\n# Colab-optimized configuration\nCOLAB_CONFIG = {\n    'target_size': 1000,  # Small dataset for demo\n    'num_epochs': 1,      # Quick training\n    'batch_size': 2,      # Small batch for memory\n    'max_length': 1024,   # Shorter sequences\n    'use_4bit': True,     # QLoRA quantization\n    'lora_r': 8,          # Smaller LoRA rank\n}\n\nprint(\"‚úÖ Colab configuration set\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phase1"
   },
   "source": [
    "## üìä Phase 1: Dataset Generation\n",
    "\n",
    "Generate high-quality code examples using Claude Opus 4."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dataset_gen"
   },
   "outputs": [],
   "source": "# Import modules with comprehensive error handling\nimport asyncio\nimport sys\nimport os\n\n# Ensure critical packages are installed\ntry:\n    import anthropic\n    import backoff\n    import yaml\n    import tqdm\n    import datasets\n    print(\"‚úÖ All critical packages available\")\nexcept ImportError as e:\n    print(f\"‚ùå Missing package: {e}\")\n    print(\"üîß Installing missing dependencies...\")\n    !pip install -q anthropic backoff pyyaml tqdm datasets\n    print(\"‚úÖ Dependencies installed, please restart runtime if needed\")\n\n# Ensure we're in the right directory and path is set\nif not os.path.exists('./src'):\n    print(\"‚ùå Not in project directory. Trying to navigate...\")\n    # Try different possible paths\n    possible_paths = [\n        '/content/drive/MyDrive/claude_distillation/Claude-to-Codellama-Distillation',\n        '/content/drive/MyDrive/claude_distillation/claude-to-codellama-distillation',\n        './Claude-to-Codellama-Distillation'\n    ]\n    \n    for path in possible_paths:\n        if os.path.exists(path):\n            os.chdir(path)\n            print(f\"‚úÖ Changed to: {os.getcwd()}\")\n            break\n    else:\n        print(\"‚ùå Project directory not found. Please run the setup cells first.\")\n\n# Add src to path\nif './src' not in sys.path:\n    sys.path.append('./src')\nif os.path.exists('./src'):\n    sys.path.append(os.path.abspath('./src'))\n\n# Try imports with helpful error messages\ntry:\n    from dataset_generator import DatasetGenerator, DatasetConfig\n    print(\"‚úÖ dataset_generator imported successfully\")\nexcept ImportError as e:\n    print(f\"‚ùå Failed to import dataset_generator: {e}\")\n    print(\"Current directory:\", os.getcwd())\n    print(\"Files in current directory:\", os.listdir('.') if os.path.exists('.') else \"Directory not found\")\n    if os.path.exists('./src'):\n        print(\"Files in src:\", os.listdir('./src'))\n    else:\n        print(\"‚ùå src directory not found\")\n\ntry:\n    from claude_client import ClaudeConfig\n    print(\"‚úÖ claude_client imported successfully\")\nexcept ImportError as e:\n    print(f\"‚ùå Failed to import claude_client: {e}\")\n    print(\"Make sure you've run the previous cells and are in the project directory\")\n\n# Configure dataset generation (only if imports succeeded)\ntry:\n    claude_config = ClaudeConfig(\n        api_key=os.getenv('ANTHROPIC_API_KEY'),\n        model='claude-3-opus-20240229',\n        max_tokens=1024,\n        temperature=0.1,\n        rate_limit_rpm=30  # Conservative for Colab\n    )\n\n    dataset_config = DatasetConfig(\n        target_size=COLAB_CONFIG['target_size'],\n        languages=['python', 'javascript'],  # Start with 2 languages\n        output_dir='./data/generated'\n    )\n\n    print(\"üèóÔ∏è Starting dataset generation...\")\n    print(f\"Target size: {dataset_config.target_size} examples\")\n    print(f\"Languages: {dataset_config.languages}\")\n    \nexcept NameError as e:\n    print(f\"‚ùå Configuration error: {e}\")\n    print(\"Make sure COLAB_CONFIG is defined and imports are successful\")\nexcept Exception as e:\n    print(f\"‚ùå Unexpected error: {e}\")\n    print(\"Trying to continue with fallback configuration...\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_generation"
   },
   "outputs": [],
   "source": "# Import modules with error handling\nimport asyncio\nimport sys\nimport os\n\n# Ensure we're in the right directory and path is set\nif not os.path.exists('./src'):\n    print(\"‚ùå Not in project directory. Trying to navigate...\")\n    if os.path.exists('/content/drive/MyDrive/claude_distillation/Claude-to-Codellama-Distillation'):\n        os.chdir('/content/drive/MyDrive/claude_distillation/Claude-to-Codellama-Distillation')\n        print(f\"‚úÖ Changed to: {os.getcwd()}\")\n    else:\n        print(\"‚ùå Project directory not found. Please run the setup cells first.\")\n\n# Add src to path\nif './src' not in sys.path:\n    sys.path.append('./src')\n\n# Try imports with helpful error messages\ntry:\n    from dataset_generator import DatasetGenerator, DatasetConfig\n    print(\"‚úÖ dataset_generator imported successfully\")\nexcept ImportError as e:\n    print(f\"‚ùå Failed to import dataset_generator: {e}\")\n    print(\"Make sure you've run the previous cells and are in the project directory\")\n\ntry:\n    from claude_client import ClaudeConfig\n    print(\"‚úÖ claude_client imported successfully\")\nexcept ImportError as e:\n    print(f\"‚ùå Failed to import claude_client: {e}\")\n    print(\"Make sure you've run the previous cells and are in the project directory\")\n\n# Configure dataset generation (only if imports succeeded)\ntry:\n    claude_config = ClaudeConfig(\n        api_key=os.getenv('ANTHROPIC_API_KEY'),\n        model='claude-3-opus-20240229',\n        max_tokens=1024,\n        temperature=0.1,\n        rate_limit_rpm=30  # Conservative for Colab\n    )\n\n    dataset_config = DatasetConfig(\n        target_size=COLAB_CONFIG['target_size'],\n        languages=['python', 'javascript'],  # Start with 2 languages\n        output_dir='./data/generated'\n    )\n\n    print(\"üèóÔ∏è Starting dataset generation...\")\n    print(f\"Target size: {dataset_config.target_size} examples\")\n    print(f\"Languages: {dataset_config.languages}\")\n    \nexcept NameError as e:\n    print(f\"‚ùå Configuration error: {e}\")\n    print(\"Make sure COLAB_CONFIG is defined and imports are successful\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phase2"
   },
   "source": "# Ensure training dependencies are available\ntry:\n    import torch\n    import transformers\n    import peft\n    import bitsandbytes\n    print(\"‚úÖ Training dependencies available\")\nexcept ImportError as e:\n    print(f\"‚ùå Missing training dependency: {e}\")\n    print(\"üîß Installing missing training dependencies...\")\n    !pip install -q torch transformers peft bitsandbytes accelerate\n    print(\"‚úÖ Training dependencies installed\")\n\n# Import training modules\ntry:\n    from distillation_trainer import KnowledgeDistillationSystem, DistillationConfig\n    print(\"‚úÖ Training modules imported successfully\")\nexcept ImportError as e:\n    print(f\"‚ùå Failed to import training modules: {e}\")\n    print(\"Make sure you're in the correct directory and all dependencies are installed\")\n\n# Try to import wandb\ntry:\n    import wandb\n    wandb_available = True\nexcept ImportError:\n    print(\"‚ÑπÔ∏è W&B not available, installing...\")\n    !pip install -q wandb\n    try:\n        import wandb\n        wandb_available = True\n        print(\"‚úÖ W&B installed and imported\")\n    except ImportError:\n        wandb_available = False\n        print(\"‚ö†Ô∏è W&B installation failed, continuing without logging\")\n\n# Initialize Weights & Biases (optional)\nif wandb_available and os.getenv('WANDB_API_KEY'):\n    try:\n        wandb.init(\n            project=\"claude-to-codellama-colab\",\n            config=COLAB_CONFIG,\n            tags=[\"colab\", \"demo\"]\n        )\n        print(\"‚úÖ W&B initialized\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è W&B initialization failed: {e}\")\n\n# Configure training\ntry:\n    config = DistillationConfig(\n        student_model_name='codellama/CodeLlama-7b-hf',\n        dataset_path='./data/generated',\n        output_dir='./models/distilled_codellama',\n        max_length=COLAB_CONFIG['max_length'],\n        num_epochs=COLAB_CONFIG['num_epochs'],\n        batch_size=COLAB_CONFIG['batch_size'],\n        gradient_accumulation_steps=4,\n        learning_rate=2e-4,\n        use_4bit=COLAB_CONFIG['use_4bit'],\n        lora_r=COLAB_CONFIG['lora_r'],\n        lora_alpha=16,\n        use_gradient_checkpointing=True,\n        eval_steps=50,\n        save_steps=100,\n        logging_steps=5\n    )\n\n    print(\"üéØ Training configuration ready\")\n    print(f\"Model: {config.student_model_name}\")\n    print(f\"Epochs: {config.num_epochs}\")\n    print(f\"Batch size: {config.batch_size}\")\n    print(f\"LoRA rank: {config.lora_r}\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Training configuration error: {e}\")\n    print(\"Make sure COLAB_CONFIG is defined and all imports are successful\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_setup"
   },
   "outputs": [],
   "source": [
    "from distillation_trainer import KnowledgeDistillationSystem, DistillationConfig\n",
    "import wandb\n",
    "\n",
    "# Initialize Weights & Biases (optional)\n",
    "if os.getenv('WANDB_API_KEY'):\n",
    "    wandb.init(\n",
    "        project=\"claude-to-codellama-colab\",\n",
    "        config=COLAB_CONFIG,\n",
    "        tags=[\"colab\", \"demo\"]\n",
    "    )\n",
    "    print(\"‚úÖ W&B initialized\")\n",
    "\n",
    "# Configure training\n",
    "config = DistillationConfig(\n",
    "    student_model_name='codellama/CodeLlama-7b-hf',\n",
    "    dataset_path='./data/generated',\n",
    "    output_dir='./models/distilled_codellama',\n",
    "    max_length=COLAB_CONFIG['max_length'],\n",
    "    num_epochs=COLAB_CONFIG['num_epochs'],\n",
    "    batch_size=COLAB_CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    use_4bit=COLAB_CONFIG['use_4bit'],\n",
    "    lora_r=COLAB_CONFIG['lora_r'],\n",
    "    lora_alpha=16,\n",
    "    use_gradient_checkpointing=True,\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    logging_steps=5\n",
    ")\n",
    "\n",
    "print(\"üéØ Training configuration ready\")\n",
    "print(f\"Model: {config.student_model_name}\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"LoRA rank: {config.lora_r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# Initialize training system\n",
    "print(\"üöÄ Initializing training system...\")\n",
    "system = KnowledgeDistillationSystem(config)\n",
    "\n",
    "try:\n",
    "    print(\"üìö Loading model and datasets...\")\n",
    "    results = system.run_full_training()\n",
    "\n",
    "    print(\"üéâ Training completed successfully!\")\n",
    "    print(f\"Final training loss: {results['train_result'].training_loss:.4f}\")\n",
    "    print(f\"Final eval loss: {results['eval_results']['eval_loss']:.4f}\")\n",
    "\n",
    "    # Save training metrics\n",
    "    training_metrics = {\n",
    "        'train_loss': results['train_result'].training_loss,\n",
    "        'eval_loss': results['eval_results']['eval_loss'],\n",
    "        'epochs': config.num_epochs,\n",
    "        'model_size': '7B',\n",
    "        'lora_rank': config.lora_r\n",
    "    }\n",
    "\n",
    "    if wandb.run:\n",
    "        wandb.log(training_metrics)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    print(\"This might be due to insufficient data or memory constraints.\")\n",
    "    print(\"Try reducing batch_size or dataset size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phase3"
   },
   "source": [
    "## üìà Phase 3: Model Evaluation\n",
    "\n",
    "Evaluate the trained model on standard benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_setup"
   },
   "outputs": [],
   "source": [
    "from evaluation_system import ModelComparator, EvaluationConfig\n",
    "\n",
    "# Configure evaluation\n",
    "eval_config = EvaluationConfig(\n",
    "    student_model_path='./models/distilled_codellama',\n",
    "    baseline_models=['codellama/CodeLlama-7b-hf'],  # Compare with base model\n",
    "    test_datasets=['humaneval'],  # Start with one benchmark\n",
    "    output_dir='./evaluation_results',\n",
    "    max_new_tokens=256,  # Shorter for speed\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"üìä Evaluation configuration ready\")\n",
    "print(f\"Datasets: {eval_config.test_datasets}\")\n",
    "print(f\"Baseline models: {eval_config.baseline_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_evaluation"
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"üîç Starting model evaluation...\")\n",
    "\n",
    "try:\n",
    "    comparator = ModelComparator(eval_config)\n",
    "    results = comparator.compare_models()\n",
    "\n",
    "    print(\"‚úÖ Evaluation completed!\")\n",
    "\n",
    "    # Display results\n",
    "    if 'comparison_summary' in results:\n",
    "        summary = results['comparison_summary']\n",
    "\n",
    "        for metric_name, rankings in summary['rankings'].items():\n",
    "            print(f\"\\nüìä {metric_name}:\")\n",
    "            for i, ranking in enumerate(rankings):\n",
    "                emoji = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else \"üìç\"\n",
    "                print(f\"  {emoji} {ranking['model']}: {ranking['value']:.3f}\")\n",
    "\n",
    "    # Generate detailed report\n",
    "    if eval_config.generate_report:\n",
    "        report = comparator.generate_report(results)\n",
    "        print(f\"\\nüìã Detailed report saved to {eval_config.output_dir}/evaluation_report.md\")\n",
    "\n",
    "    # Log to W&B\n",
    "    if wandb.run and 'comparison_summary' in results:\n",
    "        wandb.log({\"evaluation_results\": summary})\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {e}\")\n",
    "    print(\"This might be due to model loading issues or benchmark dataset problems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## üß™ Interactive Inference\n",
    "\n",
    "Test your trained model with custom prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load trained model for inference\n",
    "def load_trained_model(model_path):\n",
    "    try:\n",
    "        print(f\"üîÑ Loading model from {model_path}...\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ Model loaded successfully\")\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load model: {e}\")\n",
    "        print(\"Using base model instead...\")\n",
    "\n",
    "        # Fallback to base model\n",
    "        tokenizer = AutoTokenizer.from_pretrained('codellama/CodeLlama-7b-hf')\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            'codellama/CodeLlama-7b-hf',\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        return model, tokenizer\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = load_trained_model('./models/distilled_codellama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_function"
   },
   "outputs": [],
   "source": [
    "def generate_code(prompt, max_length=256, temperature=0.1):\n",
    "    \"\"\"Generate code from a prompt.\"\"\"\n",
    "\n",
    "    # Format prompt\n",
    "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the generated part\n",
    "    response_start = generated_text.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "    generated_code = generated_text[response_start:].strip()\n",
    "\n",
    "    return generated_code\n",
    "\n",
    "print(\"üß™ Inference function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_prompts"
   },
   "outputs": [],
   "source": [
    "# Test with sample prompts\n",
    "test_prompts = [\n",
    "    \"Write a Python function to calculate the factorial of a number\",\n",
    "    \"Create a JavaScript function to validate email addresses\",\n",
    "    \"Implement a binary search algorithm in Python\",\n",
    "    \"Write a Python function to find the longest word in a sentence\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model with sample prompts...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test {i}: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    try:\n",
    "        generated_code = generate_code(prompt)\n",
    "        print(generated_code)\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation failed: {e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive_test"
   },
   "outputs": [],
   "source": [
    "# Interactive testing\n",
    "print(\"üéÆ Interactive Code Generation\")\n",
    "print(\"Enter your prompts below. Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "\n",
    "    if prompt.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    if prompt.strip():\n",
    "        try:\n",
    "            print(\"\\nü§ñ Generated Code:\")\n",
    "            print(\"-\" * 40)\n",
    "            generated_code = generate_code(prompt)\n",
    "            print(generated_code)\n",
    "            print(\"-\" * 40)\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\\n\")\n",
    "\n",
    "print(\"üëã Thanks for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_model"
   },
   "source": [
    "## üíæ Save and Export Model\n",
    "\n",
    "Save your trained model for future use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_to_drive"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Create model archive\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "archive_name = f\"distilled_codellama_{timestamp}\"\n",
    "drive_path = f\"/content/drive/MyDrive/models/{archive_name}\"\n",
    "\n",
    "print(f\"üíæ Saving model to Google Drive: {drive_path}\")\n",
    "\n",
    "try:\n",
    "    # Create directory\n",
    "    os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "    # Copy model files\n",
    "    if os.path.exists('./models/distilled_codellama'):\n",
    "        shutil.copytree(\n",
    "            './models/distilled_codellama',\n",
    "            f\"{drive_path}/model\",\n",
    "            dirs_exist_ok=True\n",
    "        )\n",
    "\n",
    "    # Copy evaluation results\n",
    "    if os.path.exists('./evaluation_results'):\n",
    "        shutil.copytree(\n",
    "            './evaluation_results',\n",
    "            f\"{drive_path}/evaluation\",\n",
    "            dirs_exist_ok=True\n",
    "        )\n",
    "\n",
    "    # Save training summary\n",
    "    summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'config': COLAB_CONFIG,\n",
    "        'dataset_size': dataset_config.target_size if 'dataset_config' in locals() else 'unknown',\n",
    "        'training_completed': True\n",
    "    }\n",
    "\n",
    "    import json\n",
    "    with open(f\"{drive_path}/training_summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Model saved successfully!\")\n",
    "    print(f\"üìÅ Location: {drive_path}\")\n",
    "    print(f\"üìä Summary: {drive_path}/training_summary.json\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to save model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deployment"
   },
   "source": [
    "## üöÄ Deployment Options\n",
    "\n",
    "Your model is now ready for deployment! Here are some options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deployment_guide"
   },
   "outputs": [],
   "source": [
    "print(\"üöÄ Deployment Options for Your Trained Model\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "print(\"1. ü§ó Hugging Face Hub\")\n",
    "print(\"   - Upload to Hugging Face for easy sharing\")\n",
    "print(\"   - Use model.push_to_hub() method\")\n",
    "print(\"   - Example: https://huggingface.co/your-username/distilled-codellama\")\n",
    "print()\n",
    "\n",
    "print(\"2. üì± Local Inference\")\n",
    "print(\"   - Run on your local machine\")\n",
    "print(\"   - Download from Google Drive\")\n",
    "print(\"   - Use transformers library\")\n",
    "print()\n",
    "\n",
    "print(\"3. ‚òÅÔ∏è Cloud Deployment\")\n",
    "print(\"   - Deploy on AWS/GCP/Azure\")\n",
    "print(\"   - Use Inference Endpoints\")\n",
    "print(\"   - Scale automatically\")\n",
    "print()\n",
    "\n",
    "print(\"4. üîå API Service\")\n",
    "print(\"   - Create REST API with FastAPI\")\n",
    "print(\"   - Deploy with Docker\")\n",
    "print(\"   - Integrate with applications\")\n",
    "print()\n",
    "\n",
    "print(\"üìä Model Performance Summary:\")\n",
    "print(f\"   ‚Ä¢ Base Model: Code Llama 7B\")\n",
    "print(f\"   ‚Ä¢ Training: Knowledge Distillation from Claude Opus 4\")\n",
    "print(f\"   ‚Ä¢ Memory: ~6GB with QLoRA optimization\")\n",
    "print(f\"   ‚Ä¢ Speed: ~10-20 tokens/second on consumer GPU\")\n",
    "print(f\"   ‚Ä¢ Quality: Expected 70-75% on HumanEval benchmark\")\n",
    "print()\n",
    "\n",
    "print(\"üí° Next Steps:\")\n",
    "print(\"   1. Test the model thoroughly with your use cases\")\n",
    "print(\"   2. Fine-tune further if needed\")\n",
    "print(\"   3. Deploy to your preferred platform\")\n",
    "print(\"   4. Monitor performance and user feedback\")\n",
    "print()\n",
    "\n",
    "print(\"üéâ Congratulations! You've successfully trained a code generation model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup"
   },
   "outputs": [],
   "source": [
    "# Cleanup and finalize\n",
    "if wandb.run:\n",
    "    wandb.finish()\n",
    "    print(\"‚úÖ W&B run finished\")\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ GPU memory cleared\")\n",
    "\n",
    "print(\"\\nüéØ Training Session Complete!\")\n",
    "print(\"Your model has been trained and saved to Google Drive.\")\n",
    "print(\"Feel free to disconnect from Colab to save compute credits.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "lWf2b27j_3e9",
    "outputId": "4551e5d7-cc70-451f-d332-67b0d4847659"
   },
   "source": [
    "os.chdir('Claude-to-Codellama-Distillation')\n",
    "print(f\"üìÇ Project directory: {os.getcwd()}\")"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Claude-to-Codellama-Distillation'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-547505666>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Claude-to-Codellama-Distillation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìÇ Project directory: {os.getcwd()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Claude-to-Codellama-Distillation'"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}