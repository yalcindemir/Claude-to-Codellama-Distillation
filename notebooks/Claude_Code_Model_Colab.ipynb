{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"view-in-github\",\n",
    "    \"colab_type\": \"text\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"<a href=\\\"https://colab.research.google.com/github/yalcindemir/Claude-to-Codellama-Distillation/blob/main/notebooks/Claude_Code_Model_Colab.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"header\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# üöÄ Claude-to-CodeLlama Knowledge Distillation\n",
    "     New\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Transform Claude Opus 4's Superior Code Generation into an Accessible 7B Model**\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a complete end-to-end implementation of knowledge distillation from Claude Opus 4 to Code Llama 7B.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üìã Features\\n\",\n",
    "    \"- üß† **Teacher-Student Learning**: Claude Opus 4 ‚Üí Code Llama 7B\\n\",\n",
    "    \"- üí∞ **Cost Effective**: ~$50-100 for Colab Pro training\\n\",\n",
    "    \"- ‚ö° **Memory Efficient**: QLoRA optimization for 6GB GPU\\n\",\n",
    "    \"- üìä **Comprehensive Evaluation**: HumanEval and MBPP benchmarks\\n\",\n",
    "    \"- üîß **Production Ready**: Save and deploy your trained model\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üéØ Expected Results\\n\",\n",
    "    \"- **HumanEval**: 70-75% pass@1 (vs 33.5% baseline)\\n\",\n",
    "    \"- **MBPP**: 65-70% pass@1 (vs 41.4% baseline)\\n\",\n",
    "    \"- **Training Time**: 4-6 hours on Colab Pro\\n\",\n",
    "    \"- **Total Cost**: ~$60-80 including API calls\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": \"# üöÄ Quick Dependency Installation\\n# Run this cell first if you encounter import errors\\n\\nprint(\\\"üîß Installing critical dependencies for Colab...\\\")\\n\\n# Core dependencies that are commonly missing\\ncritical_deps = [\\n    \\\"anthropic>=0.25.0\\\",\\n    \\\"backoff>=2.2.1\\\", \\n    \\\"pyyaml>=6.0\\\",\\n    \\\"tqdm>=4.65.0\\\",\\n    \\\"datasets>=2.14.0\\\",\\n    \\\"bitsandbytes>=0.41.0\\\",\\n    \\\"transformers>=4.35.0\\\",\\n    \\\"accelerate>=0.24.0\\\",\\n    \\\"peft>=0.6.0\\\"\\n]\\n\\nfor dep in critical_deps:\\n    try:\\n        !pip install -q {dep}\\n        print(f\\\"‚úÖ {dep}\\\")\\n    except Exception as e:\\n        print(f\\\"‚ùå Failed to install {dep}: {e}\\\")\\n\\nprint(\\\"üéØ Critical dependencies installation complete!\\\")\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"setup\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üõ†Ô∏è Setup Environment\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, let's set up the environment and install dependencies.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"install\",\n",
    "    \"outputId\": \"7b10ccf4-bb2c-4130-9882-b9eba2e14baf\",\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"# Install essential dependencies immediately\\nprint(\\\"üîß Installing essential dependencies...\\\")\\n!pip install -q anthropic backoff pyyaml tqdm datasets bitsandbytes\\n\\n# Clone repository with correct URL\\nif not os.path.exists('Claude-to-Codellama-Distillation'):\\n    !git clone https://github.com/yalcindemir/Claude-to-Codellama-Distillation.git\\n    print(\\\"‚úÖ Repository cloned\\\")\\nelse:\\n    print(\\\"‚úÖ Repository already exists\\\")\\n\\n# Change to the correct directory\\nos.chdir('Claude-to-Codellama-Distillation')\\nprint(f\\\"üìÇ Project directory: {os.getcwd()}\\\")\\n\\n# Install the package in editable mode to fix imports\\ntry:\\n    # Try editable install first\\n    result = !pip install -e . 2>&1\\n    if \\\"error\\\" in ' '.join(result).lower():\\n        print(\\\"‚ö†Ô∏è Editable install failed, trying manual path setup...\\\")\\n        raise Exception(\\\"Setup failed\\\")\\n    else:\\n        print(\\\"‚úÖ Package installed successfully\\\")\\nexcept Exception as e:\\n    print(f\\\"‚ö†Ô∏è Package installation failed: {e}\\\")\\n    print(\\\"üìÅ Using manual path setup instead...\\\")\\n    \\n    # Manual path setup as fallback\\n    import sys\\n    import os\\n    \\n    # Add src directory to Python path\\n    src_path = os.path.join(os.getcwd(), 'src')\\n    if src_path not in sys.path:\\n        sys.path.insert(0, src_path)\\n    \\n    # Verify the path setup\\n    if os.path.exists('./src'):\\n        print(\\\"‚úÖ Manual path setup completed\\\")\\n        print(f\\\"üìÅ src directory found: {os.path.exists('./src')}\\\")\\n        src_files = [f for f in os.listdir('./src') if f.endswith('.py')]\\n        print(f\\\"üìÑ Python files in src: {src_files}\\\")\\n    else:\\n        print(\\\"‚ùå src directory not found\\\")\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"clone\",\n",
    "    \"outputId\": \"d75330b7-cd80-49ad-b08a-d8c0eb7f2058\",\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"# Install essential dependencies first\\n!pip install anthropic>=0.25.0\\n!pip install backoff>=2.2.1\\n!pip install pyyaml>=6.0\\n!pip install tqdm>=4.65.0\\n!pip install datasets>=2.14.0\\n!pip install bitsandbytes>=0.41.0\\n\\n# Install core ML dependencies\\n!pip install torch>=2.0.0\\n!pip install transformers>=4.35.0\\n!pip install accelerate>=0.24.0\\n!pip install peft>=0.6.0\\n\\n# Install evaluation dependencies\\n!pip install evaluate>=0.4.0\\n!pip install scikit-learn>=1.3.0\\n\\n# Try to install from Colab-specific requirements if available\\nimport os\\nif os.path.exists('requirements-colab.txt'):\\n    !pip install -r requirements-colab.txt\\n    print(\\\"‚úÖ Colab requirements installed\\\")\\nelif os.path.exists('requirements.txt'):\\n    # Install core packages from requirements.txt, skip problematic ones\\n    !pip install -r requirements.txt --no-deps --force-reinstall anthropic backoff pyyaml tqdm datasets transformers\\n    print(\\\"‚úÖ Core requirements from file installed\\\")\\nelse:\\n    print(\\\"‚ÑπÔ∏è No requirements file found, using manual installation\\\")\\n\\n# Additional Colab-specific packages\\n!pip install google-colab wandb\\n\\nprint(\\\"‚úÖ All dependencies installed successfully\\\")\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"requirements\",\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"outputId\": \"ad9264e6-3b48-4849-95ed-93bf8b2f0092\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"# Install essential dependencies first\\n!pip install anthropic>=0.25.0\\n!pip install backoff>=2.2.1\\n!pip install pyyaml>=6.0\\n!pip install tqdm>=4.65.0\\n!pip install datasets>=2.14.0\\n!pip install bitsandbytes>=0.41.0\\n\\n# Install core ML dependencies\\n!pip install torch>=2.0.0\\n!pip install transformers>=4.35.0\\n!pip install accelerate>=0.24.0\\n!pip install peft>=0.6.0\\n\\n# Install evaluation dependencies\\n!pip install evaluate>=0.4.0\\n!pip install scikit-learn>=1.3.0\\n\\n# Install requirements from file (if exists)\\nimport os\\nif os.path.exists('requirements.txt'):\\n    !pip install -r requirements.txt\\n    print(\\\"‚úÖ Requirements from file installed\\\")\\nelse:\\n    print(\\\"‚ÑπÔ∏è No requirements.txt found, using manual installation\\\")\\n\\n# Additional Colab-specific packages\\n!pip install google-colab\\n\\nprint(\\\"‚úÖ All dependencies installed successfully\\\")\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"config\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üîë Configuration\\n\",\n",
    "    \"\\n\",\n",
    "    \"Set up your API keys and configuration.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 26,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"api_keys\",\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"outputId\": \"d37b814a-6ed0-4a07-f010-31d0dc091c1d\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"output_type\": \"stream\",\n",
    "     \"name\": \"stdout\",\n",
    "     \"text\": [\n",
    "      \"üîë Setting up API keys...\\n\",\n",
    "      \"‚úÖ Claude API key already set\\n\",\n",
    "      \"Enter your W&B API key (optional, press Enter to skip): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\n\",\n",
    "      \"‚úÖ W&B API key set\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"from getpass import getpass\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set API keys\\n\",\n",
    "    \"print(\\\"üîë Setting up API keys...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Claude API key (required)\\n\",\n",
    "    \"if not os.getenv('ANTHROPIC_API_KEY'):\\n\",\n",
    "    \"    anthropic_key = getpass('Enter your Anthropic API key: ')\\n\",\n",
    "    \"    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\\n\",\n",
    "    \"    print(\\\"‚úÖ Claude API key set\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚úÖ Claude API key already set\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Weights & Biases (optional)\\n\",\n",
    "    \"if not os.getenv('WANDB_API_KEY'):\\n\",\n",
    "    \"    wandb_key = getpass('Enter your W&B API key (optional, press Enter to skip): ')\\n\",\n",
    "    \"    if wandb_key:\\n\",\n",
    "    \"        os.environ['WANDB_API_KEY'] = wandb_key\\n\",\n",
    "    \"        print(\\\"‚úÖ W&B API key set\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"‚è≠Ô∏è W&B skipped\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"‚úÖ W&B API key already set\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"colab_config\",\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"outputId\": \"eb0c886c-bac2-402f-b4fc-efd3d22c3839\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"# Colab-specific configuration\\nimport torch\\nimport sys\\n\\n# Add src to path (multiple ways for reliability)\\nsys.path.append('./src')\\nsys.path.append('/content/drive/MyDrive/claude_distillation/Claude-to-Codellama-Distillation/src')\\n\\n# Verify we're in the right directory\\nimport os\\nprint(f\\\"üìÇ Current directory: {os.getcwd()}\\\")\\nif os.path.exists('./src'):\\n    print(\\\"‚úÖ src directory found\\\")\\n    # List Python files in src\\n    src_files = [f for f in os.listdir('./src') if f.endswith('.py')]\\n    print(f\\\"üìÑ Python files in src: {src_files}\\\")\\nelse:\\n    print(\\\"‚ùå src directory not found - need to navigate to project directory first\\\")\\n\\n# Check GPU\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nprint(f\\\"üéÆ Device: {device}\\\")\\n\\nif torch.cuda.is_available():\\n    gpu_name = torch.cuda.get_device_name(0)\\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\\n    print(f\\\"GPU: {gpu_name} ({gpu_memory:.1f}GB)\\\")\\nelse:\\n    print(\\\"‚ö†Ô∏è No GPU available - training will be very slow!\\\")\\n\\n# Colab-optimized configuration\\nCOLAB_CONFIG = {\\n    'target_size': 1000,  # Small dataset for demo\\n    'num_epochs': 1,      # Quick training\\n    'batch_size': 2,      # Small batch for memory\\n    'max_length': 1024,   # Shorter sequences\\n    'use_4bit': True,     # QLoRA quantization\\n    'lora_r': 8,          # Smaller LoRA rank\\n}\\n\\nprint(\\\"‚úÖ Colab configuration set\\\")\",\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"phase1\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üìä Phase 1: Dataset Generation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Generate high-quality code examples using Claude Opus 4.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"dataset_gen\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"# Import modules with comprehensive error handling\\nimport asyncio\\nimport sys\\nimport os\\n\\n# Ensure critical packages are installed\\ntry:\\n    import anthropic\\n    import backoff\\n    import yaml\\n    import tqdm\\n    import datasets\\n    print(\\\"‚úÖ All critical packages available\\\")\\nexcept ImportError as e:\\n    print(f\\\"‚ùå Missing package: {e}\\\")\\n    print(\\\"üîß Installing missing dependencies...\\\")\\n    !pip install -q anthropic backoff pyyaml tqdm datasets\\n    print(\\\"‚úÖ Dependencies installed, please restart runtime if needed\\\")\\n\\n# Ensure we're in the right directory and path is set\\nif not os.path.exists('./src'):\\n    print(\\\"‚ùå Not in project directory. Trying to navigate...\\\")\\n    # Try different possible paths\\n    possible_paths = [\\n        '/content/drive/MyDrive/claude_distillation/Claude-to-Codellama-Distillation',\\n        '/content/drive/MyDrive/claude_distillation/claude-to-codellama-distillation',\\n        './Claude-to-Codellama-Distillation'\\n    ]\\n    \\n    for path in possible_paths:\\n        if os.path.exists(path):\\n            os.chdir(path)\\n            print(f\\\"‚úÖ Changed to: {os.getcwd()}\\\")\\n            break\\n    else:\\n        print(\\\"‚ùå Project directory not found. Please run the setup cells first.\\\")\\n\\n# Add src to path\\nif './src' not in sys.path:\\n    sys.path.append('./src')\\nif os.path.exists('./src'):\\n    sys.path.append(os.path.abspath('./src'))\\n\\n# Try imports with helpful error messages\\ntry:\\n    from dataset_generator import DatasetGenerator, DatasetConfig\\n    print(\\\"‚úÖ dataset_generator imported successfully\\\")\\nexcept ImportError as e:\\n    print(f\\\"‚ùå Failed to import dataset_generator: {e}\\\")\\n    print(\\\"Current directory:\\\", os.getcwd())\\n    print(\\\"Files in current directory:\\\", os.listdir('.') if os.path.exists('.') else \\\"Directory not found\\\")\\n    if os.path.exists('./src'):\\n        print(\\\"Files in src:\\\", os.listdir('./src'))\\n    else:\\n        print(\\\"‚ùå src directory not found\\\")\\n\\ntry:\\n    from claude_client import ClaudeConfig\\n    print(\\\"‚úÖ claude_client imported successfully\\\")\\nexcept ImportError as e:\\n    print(f\\\"‚ùå Failed to import claude_client: {e}\\\")\\n    print(\\\"Make sure you've run the previous cells and are in the project directory\\\")\\n\\n# Configure dataset generation (only if imports succeeded)\\ntry:\\n    claude_config = ClaudeConfig(\\n        api_key=os.getenv('ANTHROPIC_API_KEY'),\\n        model='claude-3-opus-20240229',\\n        max_tokens=1024,\\n        temperature=0.1,\\n        rate_limit_rpm=30  # Conservative for Colab\\n    )\\n\\n    dataset_config = DatasetConfig(\\n        target_size=COLAB_CONFIG['target_size'],\\n        languages=['python', 'javascript'],  # Start with 2 languages\\n        output_dir='./data/generated'\\n    )\\n\\n    print(\\\"üèóÔ∏è Starting dataset generation...\\\")\\n    print(f\\\"Target size: {dataset_config.target_size} examples\\\")\\n    print(f\\\"Languages: {dataset_config.languages}\\\")\\n    \\nexcept NameError as e:\\n    print(f\\\"‚ùå Configuration error: {e}\\\")\\n    print(\\\"Make sure COLAB_CONFIG is defined and imports are successful\\\")\\nexcept Exception as e:\\n    print(f\\\"‚ùå Unexpected error: {e}\\\")\\n    print(\\\"Trying to continue with fallback configuration...\\\")\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"run_generation\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"# Import modules with error handling\\nimport asyncio\\nimport sys\\nimport os\\n\\n# Ensure we're in the right directory and path is set\\nif not os.path.exists('./src'):\\n    print(\\\"‚ùå Not in project directory. Trying to navigate...\\\")\\n    if os.path.exists('/content/drive/MyDrive/claude_distillation/Claude-to-Codellama-Distillation'):\\n        os.chdir('/content/drive/MyDrive/claude_distillation/Claude-to-Codellama-Distillation')\\n        print(f\\\"‚úÖ Changed to: {os.getcwd()}\\\")\\n    else:\\n        print(\\\"‚ùå Project directory not found. Please run the setup cells first.\\\")\\n\\n# Add src to path\\nif './src' not in sys.path:\\n    sys.path.append('./src')\\n\\n# Try imports with helpful error messages\\ntry:\\n    from dataset_generator import DatasetGenerator, DatasetConfig\\n    print(\\\"‚úÖ dataset_generator imported successfully\\\")\\nexcept ImportError as e:\\n    print(f\\\"‚ùå Failed to import dataset_generator: {e}\\\")\\n    print(\\\"Make sure you've run the previous cells and are in the project directory\\\")\\n\\ntry:\\n    from claude_client import ClaudeConfig\\n    print(\\\"‚úÖ claude_client imported successfully\\\")\\nexcept ImportError as e:\\n    print(f\\\"‚ùå Failed to import claude_client: {e}\\\")\\n    print(\\\"Make sure you've run the previous cells and are in the project directory\\\")\\n\\n# Configure dataset generation (only if imports succeeded)\\ntry:\\n    claude_config = ClaudeConfig(\\n        api_key=os.getenv('ANTHROPIC_API_KEY'),\\n        model='claude-3-opus-20240229',\\n        max_tokens=1024,\\n        temperature=0.1,\\n        rate_limit_rpm=30  # Conservative for Colab\\n    )\\n\\n    dataset_config = DatasetConfig(\\n        target_size=COLAB_CONFIG['target_size'],\\n        languages=['python', 'javascript'],  # Start with 2 languages\\n        output_dir='./data/generated'\\n    )\\n\\n    print(\\\"üèóÔ∏è Starting dataset generation...\\\")\\n    print(f\\\"Target size: {dataset_config.target_size} examples\\\")\\n    print(f\\\"Languages: {dataset_config.languages}\\\")\\n    \\nexcept NameError as e:\\n    print(f\\\"‚ùå Configuration error: {e}\\\")\\n    print(\\\"Make sure COLAB_CONFIG is defined and imports are successful\\\")\",\n",
    "   \"execution_count\": null\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"phase2\"\n",
    "   },\n",
    "   \"source\": \"# Ensure training dependencies are available\\ntry:\\n    import torch\\n    import transformers\\n    import peft\\n    import bitsandbytes\\n    print(\\\"‚úÖ Training dependencies available\\\")\\nexcept ImportError as e:\\n    print(f\\\"‚ùå Missing training dependency: {e}\\\")\\n    print(\\\"üîß Installing missing training dependencies...\\\")\\n    !pip install -q torch transformers peft bitsandbytes accelerate\\n    print(\\\"‚úÖ Training dependencies installed\\\")\\n\\n# Import training modules\\ntry:\\n    from distillation_trainer import KnowledgeDistillationSystem, DistillationConfig\\n    print(\\\"‚úÖ Training modules imported successfully\\\")\\nexcept ImportError as e:\\n    print(f\\\"‚ùå Failed to import training modules: {e}\\\")\\n    print(\\\"Make sure you're in the correct directory and all dependencies are installed\\\")\\n\\n# Try to import wandb\\ntry:\\n    import wandb\\n    wandb_available = True\\nexcept ImportError:\\n    print(\\\"‚ÑπÔ∏è W&B not available, installing...\\\")\\n    !pip install -q wandb\\n    try:\\n        import wandb\\n        wandb_available = True\\n        print(\\\"‚úÖ W&B installed and imported\\\")\\n    except ImportError:\\n        wandb_available = False\\n        print(\\\"‚ö†Ô∏è W&B installation failed, continuing without logging\\\")\\n\\n# Initialize Weights & Biases (optional)\\nif wandb_available and os.getenv('WANDB_API_KEY'):\\n    try:\\n        wandb.init(\\n            project=\\\"claude-to-codellama-colab\\\",\\n            config=COLAB_CONFIG,\\n            tags=[\\\"colab\\\", \\\"demo\\\"]\\n        )\\n        print(\\\"‚úÖ W&B initialized\\\")\\n    except Exception as e:\\n        print(f\\\"‚ö†Ô∏è W&B initialization failed: {e}\\\")\\n\\n# Configure training\\ntry:\\n    config = DistillationConfig(\\n        student_model_name='codellama/CodeLlama-7b-hf',\\n        dataset_path='./data/generated',\\n        output_dir='./models/distilled_codellama',\\n        max_length=COLAB_CONFIG['max_length'],\\n        num_epochs=COLAB_CONFIG['num_epochs'],\\n        batch_size=COLAB_CONFIG['batch_size'],\\n        gradient_accumulation_steps=4,\\n        learning_rate=2e-4,\\n        use_4bit=COLAB_CONFIG['use_4bit'],\\n        lora_r=COLAB_CONFIG['lora_r'],\\n        lora_alpha=16,\\n        use_gradient_checkpointing=True,\\n        eval_steps=50,\\n        save_steps=100,\\n        logging_steps=5\\n    )\\n\\n    print(\\\"üéØ Training configuration ready\\\")\\n    print(f\\\"Model: {config.student_model_name}\\\")\\n    print(f\\\"Epochs: {config.num_epochs}\\\")\\n    print(f\\\"Batch size: {config.batch_size}\\\")\\n    print(f\\\"LoRA rank: {config.lora_r}\\\")\\n    \\nexcept Exception as e:\\n    print(f\\\"‚ùå Training configuration error: {e}\\\")\\n    print(\\\"Make sure COLAB_CONFIG is defined and all imports are successful\\\")\",\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"training_setup\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from distillation_trainer import KnowledgeDistillationSystem, DistillationConfig\\n\",\n",
    "    \"import wandb\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize Weights & Biases (optional)\\n\",\n",
    "    \"if os.getenv('WANDB_API_KEY'):\\n\",\n",
    "    \"    wandb.init(\\n\",\n",
    "    \"        project=\\\"claude-to-codellama-colab\\\",\\n\",\n",
    "    \"        config=COLAB_CONFIG,\\n\",\n",
    "    \"        tags=[\\\"colab\\\", \\\"demo\\\"]\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    print(\\\"‚úÖ W&B initialized\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure training\\n\",\n",
    "    \"config = DistillationConfig(\\n\",\n",
    "    \"    student_model_name='codellama/CodeLlama-7b-hf',\\n\",\n",
    "    \"    dataset_path='./data/generated',\\n\",\n",
    "    \"    output_dir='./models/distilled_codellama',\\n\",\n",
    "    \"    max_length=COLAB_CONFIG['max_length'],\\n\",\n",
    "    \"    num_epochs=COLAB_CONFIG['num_epochs'],\\n\",\n",
    "    \"    batch_size=COLAB_CONFIG['batch_size'],\\n\",\n",
    "    \"    gradient_accumulation_steps=4,\\n\",\n",
    "    \"    learning_rate=2e-4,\\n\",\n",
    "    \"    use_4bit=COLAB_CONFIG['use_4bit'],\\n\",\n",
    "    \"    lora_r=COLAB_CONFIG['lora_r'],\\n\",\n",
    "    \"    lora_alpha=16,\\n\",\n",
    "    \"    use_gradient_checkpointing=True,\\n\",\n",
    "    \"    eval_steps=50,\\n\",\n",
    "    \"    save_steps=100,\\n\",\n",
    "    \"    logging_steps=5\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üéØ Training configuration ready\\\")\\n\",\n",
    "    \"print(f\\\"Model: {config.student_model_name}\\\")\\n\",\n",
    "    \"print(f\\\"Epochs: {config.num_epochs}\\\")\\n\",\n",
    "    \"print(f\\\"Batch size: {config.batch_size}\\\")\\n\",\n",
    "    \"print(f\\\"LoRA rank: {config.lora_r}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"run_training\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize training system\\n\",\n",
    "    \"print(\\\"üöÄ Initializing training system...\\\")\\n\",\n",
    "    \"system = KnowledgeDistillationSystem(config)\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    print(\\\"üìö Loading model and datasets...\\\")\\n\",\n",
    "    \"    results = system.run_full_training()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(\\\"üéâ Training completed successfully!\\\")\\n\",\n",
    "    \"    print(f\\\"Final training loss: {results['train_result'].training_loss:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Final eval loss: {results['eval_results']['eval_loss']:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Save training metrics\\n\",\n",
    "    \"    training_metrics = {\\n\",\n",
    "    \"        'train_loss': results['train_result'].training_loss,\\n\",\n",
    "    \"        'eval_loss': results['eval_results']['eval_loss'],\\n\",\n",
    "    \"        'epochs': config.num_epochs,\\n\",\n",
    "    \"        'model_size': '7B',\\n\",\n",
    "    \"        'lora_rank': config.lora_r\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if wandb.run:\\n\",\n",
    "    \"        wandb.log(training_metrics)\\n\",\n",
    "    \"\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"‚ùå Training failed: {e}\\\")\\n\",\n",
    "    \"    print(\\\"This might be due to insufficient data or memory constraints.\\\")\\n\",\n",
    "    \"    print(\\\"Try reducing batch_size or dataset size.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"phase3\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üìà Phase 3: Model Evaluation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Evaluate the trained model on standard benchmarks.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"evaluation_setup\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from evaluation_system import ModelComparator, EvaluationConfig\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure evaluation\\n\",\n",
    "    \"eval_config = EvaluationConfig(\\n\",\n",
    "    \"    student_model_path='./models/distilled_codellama',\\n\",\n",
    "    \"    baseline_models=['codellama/CodeLlama-7b-hf'],  # Compare with base model\\n\",\n",
    "    \"    test_datasets=['humaneval'],  # Start with one benchmark\\n\",\n",
    "    \"    output_dir='./evaluation_results',\\n\",\n",
    "    \"    max_new_tokens=256,  # Shorter for speed\\n\",\n",
    "    \"    temperature=0.1\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìä Evaluation configuration ready\\\")\\n\",\n",
    "    \"print(f\\\"Datasets: {eval_config.test_datasets}\\\")\\n\",\n",
    "    \"print(f\\\"Baseline models: {eval_config.baseline_models}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"run_evaluation\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Run evaluation\\n\",\n",
    "    \"print(\\\"üîç Starting model evaluation...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    comparator = ModelComparator(eval_config)\\n\",\n",
    "    \"    results = comparator.compare_models()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(\\\"‚úÖ Evaluation completed!\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Display results\\n\",\n",
    "    \"    if 'comparison_summary' in results:\\n\",\n",
    "    \"        summary = results['comparison_summary']\\n\",\n",
    "    \"\\n\",\n",
    "    \"        for metric_name, rankings in summary['rankings'].items():\\n\",\n",
    "    \"            print(f\\\"\\\\nüìä {metric_name}:\\\")\\n\",\n",
    "    \"            for i, ranking in enumerate(rankings):\\n\",\n",
    "    \"                emoji = \\\"ü•á\\\" if i == 0 else \\\"ü•à\\\" if i == 1 else \\\"ü•â\\\" if i == 2 else \\\"üìç\\\"\\n\",\n",
    "    \"                print(f\\\"  {emoji} {ranking['model']}: {ranking['value']:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Generate detailed report\\n\",\n",
    "    \"    if eval_config.generate_report:\\n\",\n",
    "    \"        report = comparator.generate_report(results)\\n\",\n",
    "    \"        print(f\\\"\\\\nüìã Detailed report saved to {eval_config.output_dir}/evaluation_report.md\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Log to W&B\\n\",\n",
    "    \"    if wandb.run and 'comparison_summary' in results:\\n\",\n",
    "    \"        wandb.log({\\\"evaluation_results\\\": summary})\\n\",\n",
    "    \"\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"‚ùå Evaluation failed: {e}\\\")\\n\",\n",
    "    \"    print(\\\"This might be due to model loading issues or benchmark dataset problems.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"inference\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üß™ Interactive Inference\\n\",\n",
    "    \"\\n\",\n",
    "    \"Test your trained model with custom prompts.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"load_model\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"from transformers import AutoTokenizer, AutoModelForCausalLM\\n\",\n",
    "    \"from peft import PeftModel\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load trained model for inference\\n\",\n",
    "    \"def load_trained_model(model_path):\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        print(f\\\"üîÑ Loading model from {model_path}...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Load tokenizer\\n\",\n",
    "    \"        tokenizer = AutoTokenizer.from_pretrained(model_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Load model\\n\",\n",
    "    \"        model = AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "    \"            model_path,\\n\",\n",
    "    \"            torch_dtype=torch.float16,\\n\",\n",
    "    \"            device_map=\\\"auto\\\"\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"        print(\\\"‚úÖ Model loaded successfully\\\")\\n\",\n",
    "    \"        return model, tokenizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"‚ùå Failed to load model: {e}\\\")\\n\",\n",
    "    \"        print(\\\"Using base model instead...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Fallback to base model\\n\",\n",
    "    \"        tokenizer = AutoTokenizer.from_pretrained('codellama/CodeLlama-7b-hf')\\n\",\n",
    "    \"        model = AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "    \"            'codellama/CodeLlama-7b-hf',\\n\",\n",
    "    \"            torch_dtype=torch.float16,\\n\",\n",
    "    \"            device_map=\\\"auto\\\"\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        return model, tokenizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the model\\n\",\n",
    "    \"model, tokenizer = load_trained_model('./models/distilled_codellama')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"inference_function\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def generate_code(prompt, max_length=256, temperature=0.1):\\n\",\n",
    "    \"    \\\"\\\"\\\"Generate code from a prompt.\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Format prompt\\n\",\n",
    "    \"    formatted_prompt = f\\\"### Instruction:\\\\n{prompt}\\\\n\\\\n### Response:\\\\n\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Tokenize\\n\",\n",
    "    \"    inputs = tokenizer(\\n\",\n",
    "    \"        formatted_prompt,\\n\",\n",
    "    \"        return_tensors=\\\"pt\\\",\\n\",\n",
    "    \"        truncation=True,\\n\",\n",
    "    \"        max_length=512\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Generate\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        outputs = model.generate(\\n\",\n",
    "    \"            **inputs,\\n\",\n",
    "    \"            max_new_tokens=max_length,\\n\",\n",
    "    \"            temperature=temperature,\\n\",\n",
    "    \"            do_sample=True,\\n\",\n",
    "    \"            top_p=0.95,\\n\",\n",
    "    \"            pad_token_id=tokenizer.eos_token_id\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Decode\\n\",\n",
    "    \"    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Extract only the generated part\\n\",\n",
    "    \"    response_start = generated_text.find(\\\"### Response:\\\\n\\\") + len(\\\"### Response:\\\\n\\\")\\n\",\n",
    "    \"    generated_code = generated_text[response_start:].strip()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return generated_code\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üß™ Inference function ready!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"test_prompts\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test with sample prompts\\n\",\n",
    "    \"test_prompts = [\\n\",\n",
    "    \"    \\\"Write a Python function to calculate the factorial of a number\\\",\\n\",\n",
    "    \"    \\\"Create a JavaScript function to validate email addresses\\\",\\n\",\n",
    "    \"    \\\"Implement a binary search algorithm in Python\\\",\\n\",\n",
    "    \"    \\\"Write a Python function to find the longest word in a sentence\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üß™ Testing model with sample prompts...\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, prompt in enumerate(test_prompts, 1):\\n\",\n",
    "    \"    print(f\\\"{'='*60}\\\")\\n\",\n",
    "    \"    print(f\\\"Test {i}: {prompt}\\\")\\n\",\n",
    "    \"    print(f\\\"{'='*60}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        generated_code = generate_code(prompt)\\n\",\n",
    "    \"        print(generated_code)\\n\",\n",
    "    \"        print()\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"‚ùå Generation failed: {e}\\\")\\n\",\n",
    "    \"        print()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"interactive_test\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Interactive testing\\n\",\n",
    "    \"print(\\\"üéÆ Interactive Code Generation\\\")\\n\",\n",
    "    \"print(\\\"Enter your prompts below. Type 'quit' to exit.\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"while True:\\n\",\n",
    "    \"    prompt = input(\\\"Enter your prompt: \\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if prompt.lower() == 'quit':\\n\",\n",
    "    \"        break\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if prompt.strip():\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            print(\\\"\\\\nü§ñ Generated Code:\\\")\\n\",\n",
    "    \"            print(\\\"-\\\" * 40)\\n\",\n",
    "    \"            generated_code = generate_code(prompt)\\n\",\n",
    "    \"            print(generated_code)\\n\",\n",
    "    \"            print(\\\"-\\\" * 40)\\n\",\n",
    "    \"            print()\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"‚ùå Error: {e}\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üëã Thanks for testing!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"save_model\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üíæ Save and Export Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"Save your trained model for future use or deployment.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"save_to_drive\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import shutil\\n\",\n",
    "    \"from datetime import datetime\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create model archive\\n\",\n",
    "    \"timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n\",\n",
    "    \"archive_name = f\\\"distilled_codellama_{timestamp}\\\"\\n\",\n",
    "    \"drive_path = f\\\"/content/drive/MyDrive/models/{archive_name}\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"üíæ Saving model to Google Drive: {drive_path}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    # Create directory\\n\",\n",
    "    \"    os.makedirs(drive_path, exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Copy model files\\n\",\n",
    "    \"    if os.path.exists('./models/distilled_codellama'):\\n\",\n",
    "    \"        shutil.copytree(\\n\",\n",
    "    \"            './models/distilled_codellama',\\n\",\n",
    "    \"            f\\\"{drive_path}/model\\\",\\n\",\n",
    "    \"            dirs_exist_ok=True\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Copy evaluation results\\n\",\n",
    "    \"    if os.path.exists('./evaluation_results'):\\n\",\n",
    "    \"        shutil.copytree(\\n\",\n",
    "    \"            './evaluation_results',\\n\",\n",
    "    \"            f\\\"{drive_path}/evaluation\\\",\\n\",\n",
    "    \"            dirs_exist_ok=True\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Save training summary\\n\",\n",
    "    \"    summary = {\\n\",\n",
    "    \"        'timestamp': timestamp,\\n\",\n",
    "    \"        'config': COLAB_CONFIG,\\n\",\n",
    "    \"        'dataset_size': dataset_config.target_size if 'dataset_config' in locals() else 'unknown',\\n\",\n",
    "    \"        'training_completed': True\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"    import json\\n\",\n",
    "    \"    with open(f\\\"{drive_path}/training_summary.json\\\", 'w') as f:\\n\",\n",
    "    \"        json.dump(summary, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"‚úÖ Model saved successfully!\\\")\\n\",\n",
    "    \"    print(f\\\"üìÅ Location: {drive_path}\\\")\\n\",\n",
    "    \"    print(f\\\"üìä Summary: {drive_path}/training_summary.json\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"‚ùå Failed to save model: {e}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"deployment\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## üöÄ Deployment Options\\n\",\n",
    "    \"\\n\",\n",
    "    \"Your model is now ready for deployment! Here are some options:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"deployment_guide\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"üöÄ Deployment Options for Your Trained Model\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"1. ü§ó Hugging Face Hub\\\")\\n\",\n",
    "    \"print(\\\"   - Upload to Hugging Face for easy sharing\\\")\\n\",\n",
    "    \"print(\\\"   - Use model.push_to_hub() method\\\")\\n\",\n",
    "    \"print(\\\"   - Example: https://huggingface.co/your-username/distilled-codellama\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"2. üì± Local Inference\\\")\\n\",\n",
    "    \"print(\\\"   - Run on your local machine\\\")\\n\",\n",
    "    \"print(\\\"   - Download from Google Drive\\\")\\n\",\n",
    "    \"print(\\\"   - Use transformers library\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"3. ‚òÅÔ∏è Cloud Deployment\\\")\\n\",\n",
    "    \"print(\\\"   - Deploy on AWS/GCP/Azure\\\")\\n\",\n",
    "    \"print(\\\"   - Use Inference Endpoints\\\")\\n\",\n",
    "    \"print(\\\"   - Scale automatically\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"4. üîå API Service\\\")\\n\",\n",
    "    \"print(\\\"   - Create REST API with FastAPI\\\")\\n\",\n",
    "    \"print(\\\"   - Deploy with Docker\\\")\\n\",\n",
    "    \"print(\\\"   - Integrate with applications\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üìä Model Performance Summary:\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Base Model: Code Llama 7B\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Training: Knowledge Distillation from Claude Opus 4\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Memory: ~6GB with QLoRA optimization\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Speed: ~10-20 tokens/second on consumer GPU\\\")\\n\",\n",
    "    \"print(f\\\"   ‚Ä¢ Quality: Expected 70-75% on HumanEval benchmark\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üí° Next Steps:\\\")\\n\",\n",
    "    \"print(\\\"   1. Test the model thoroughly with your use cases\\\")\\n\",\n",
    "    \"print(\\\"   2. Fine-tune further if needed\\\")\\n\",\n",
    "    \"print(\\\"   3. Deploy to your preferred platform\\\")\\n\",\n",
    "    \"print(\\\"   4. Monitor performance and user feedback\\\")\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üéâ Congratulations! You've successfully trained a code generation model!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"cleanup\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cleanup and finalize\\n\",\n",
    "    \"if wandb.run:\\n\",\n",
    "    \"    wandb.finish()\\n\",\n",
    "    \"    print(\\\"‚úÖ W&B run finished\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Clear GPU memory\\n\",\n",
    "    \"if torch.cuda.is_available():\\n\",\n",
    "    \"    torch.cuda.empty_cache()\\n\",\n",
    "    \"    print(\\\"‚úÖ GPU memory cleared\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüéØ Training Session Complete!\\\")\\n\",\n",
    "    \"print(\\\"Your model has been trained and saved to Google Drive.\\\")\\n\",\n",
    "    \"print(\\\"Feel free to disconnect from Colab to save compute credits.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\",\n",
    "     \"height\": 162\n",
    "    },\n",
    "    \"id\": \"lWf2b27j_3e9\",\n",
    "    \"outputId\": \"4551e5d7-cc70-451f-d332-67b0d4847659\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"os.chdir('Claude-to-Codellama-Distillation')\\n\",\n",
    "    \"print(f\\\"üìÇ Project directory: {os.getcwd()}\\\")\"\n",
    "   ],\n",
    "   \"execution_count\": 12,\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"output_type\": \"error\",\n",
    "     \"ename\": \"FileNotFoundError\",\n",
    "     \"evalue\": \"[Errno 2] No such file or directory: 'Claude-to-Codellama-Distillation'\",\n",
    "     \"traceback\": [\n",
    "      \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\n",
    "      \"\\u001b[0;31mFileNotFoundError\\u001b[0m                         Traceback (most recent call last)\",\n",
    "      \"\\u001b[0;32m<ipython-input-12-547505666>\\u001b[0m in \\u001b[0;36m<cell line: 0>\\u001b[0;34m()\\u001b[0m\\n\\u001b[0;32m----> 1\\u001b[0;31m \\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mchdir\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m'Claude-to-Codellama-Distillation'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      2\\u001b[0m \\u001b[0mprint\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34mf\\\"üìÇ Project directory: {os.getcwd()}\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n",
    "      \"\\u001b[0;31mFileNotFoundError\\u001b[0m: [Errno 2] No such file or directory: 'Claude-to-Codellama-Distillation'\"\n",
    "     ]\n",
    "    }\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"accelerator\": \"GPU\",\n",
    "  \"colab\": {\n",
    "   \"provenance\": [],\n",
    "   \"gpuType\": \"T4\",\n",
    "   \"include_colab_link\": true\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 0\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
